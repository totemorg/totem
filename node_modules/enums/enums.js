// UNCLASSIFIED 

/**
Provides a clock, data fetcher, enumerator, streamer, and getter.

@module ENUMS

@requires os
@requires cluster
@requires fs
@requires http
@requires https
@requires vm
@requires cp
@requires crypto
@requires stream
*/

/**
Extend the con prototype with the specified methods.  Array, String, Date, and Object keys are 
interpretted to extend their respective prototypes.  
@memberof Array
*/
Array.prototype.Extend = function (con) {
	this.forEach( function (proto) {
		//console.log("ext", proto.name, con);
		con.prototype[proto.name] = proto;
	});
}

const
	ENV = process.env,
	OS = require("os"),
	FS = require("fs"),
	STREAM = require("stream"),
	HTTP = require("http"),						//< http interface
	HTTPS = require("https"),					//< https interface	  
	CLUSTER = require("cluster"),				//< for tasking
	VM = require("vm"),							//< for getting
	CP = require("child_process"),				//< spawn OS shell commands
	CRYPTO = require("crypto");					//< crypto for SecureLink


// enum i/f

/**
Create a clock object with specified trace switch, every interval, on-off times, and start date.
See the clock tick method for more information.

@param {Boolean} trace tracking switch
@param {String | Float} every tick interval
@param {Float} on on-time or 0 implies infinity
@param {Float} off off-time or 0
@param {Date} start start date

*/

function Clock(trace,every,on,off,start) {
	this.next = start ? new Date( start ) : new Date();
	this.on = on || Infinity;
	this.off = off || 0;
	this.epoch = this.off;
	this.every = every;
	this.trace = trace;
	this.cycle = off ? on+off-2 : 1; //(on>=2) ? on+off-2 : 0;  // Internal clock cycle = CYCLE - 2 = ON+OFF-2.
	this.step = 1;
	this.util = on ? on/(on+off) : 0;
	
	if ( trace ) Log(this);
}

const
	{ 	
		operators,
		Copy, Each, typeOf, Stream, Log, Fetch,
	 	isArray, isObject, isString, isFunction } = ENUMS = module.exports = {
	
	Log: (...args) => console.log(">>>enum", args),

	config: opts => {
		if ( opts ) Copy( opts, ENUM, "." );
	},
	
	typeOf: obj => obj.constructor.name,
	isString: obj => typeOf(obj) == "String",
	isNumber: obj => typeOf(obj)== "Number",
	isArray: obj => typeOf(obj) == "Array",
	isKeyed: obj => Object.keys(obj).length ? true : false,
	isObject: obj => typeOf(obj) == "Object",
	isDate: obj => typeOf(obj) == "Date",
	isFunction: obj => typeOf(obj) == "Function",
	isError: obj => typeOf(obj) == "Error",
	isBoolean: obj => typeOf(obj) == "Boolean",
	isBuffer: obj => typeOf(obj) == "Buffer",
	isEmpty: opts => {
		for ( var key in opts ) return false;
		return true;
	},
	
/**
Defines parsePath operators.
*/
	operators: ["=", "<", "<=", ">", ">=", "!=", "!bin=", "!exp=", "!nlp="],

/**
Copy source hash src to target hash tar.  If the copy is shallow (deep = false), a 
Copy({...}, {}) is equivalent to new Object({...}).  In a deep copy,
(e.g. deep = "."), src keys are treated as keys into the target thusly:

	{	
		A: value,			// sets target[A] = value

		"A.B.C": value, 	// sets target[A][B][C] = value

		"A.B.C.": {			// appends X,Y to target[A][B][C]
			X:value, Y:value, ...
		},	

		OBJECT: [ 			// prototype OBJECT (Array,String,Date,Object) = method X,Y, ...
			function X() {}, 
			function Y() {}, 
		... ]

	} 

@param {Object} src source hash
@param {Object} tar target hash
@param {String} deep copy key 
@return {Object} target hash
*/
	Copy: (src,tar,deep) => {

		for (var key in src) {
			var val = src[key];

			if (deep) 
				switch (key) {
					case Array: 
						val.Extend(Array);
						break;

					case "String": 
						val.Extend(String);
						break;

					case "Date": 
						val.Extend(Date);
						break;

					case "Object": 	
						val.Extend(Object);
						break;

					/*case "Function": 
						this.callStack.push( val ); 
						break; */

					default:

						var 
							keys = key.split(deep), 
							Tar = tar,
							idx = keys[0],
							N = keys.length-1;

						for ( var n=0; n < N ;  idx = keys[++n]	) { // index to the element to set/append
							if ( idx in Tar ) {
								if ( !Tar[idx] ) Tar[idx] = new Object();
								Tar = Tar[idx];
							}

							else
								Tar = Tar[idx] = new Object(); //new Array();
						}

						if (idx)  // not null so update target
							Tar[idx] = val;

						else  // null so append to target
						if (val.constructor == Object) 
							for (var n in val) 
								Tar[n] = val[n];

						else
							Tar.push( val );
				}

			else
				tar[key] = val;
		}

		return tar;
	},

/**	 
Enumerate Object A over its keys with callback cb(key,val).

@param {Object} A source object
@param {Function} cb callback (key,val) 
*/
	Each: ( A, cb ) => {
		Object.keys(A).forEach( key => cb( key, A[key] ) );
	},
	
/**	 
Stream Array|Object|File src to callback where:

	cb( (rec,key,res) => {
		if ( res ) // still streaming 
			res( msg || undefined )  // pass undefined to bypass msg stacking

		else 
			// streaming done so key contains msg stack
	})

@param {Object | Array | String} src source object or array
@param {Function} cb callback ( rec || null, key, res ) 		
*/
	Stream: (src,opts,cb) => {
		
		var
			msgs = []; 
		
		if ( isString(src) ) 	// stream file
			src.streamFile( opts, recs => {
				if ( recs )
					recs.forEach( (rec,idx) => {
						cb( rec, idx, msg => {
							if ( msg != undefined ) 
								msgs.push( msg );
						});
					});
				
				else	// signal end
					cb(null, msgs);
			});
		
		else {	// stream array/object
			var 
				returns = 0, 
				calls = src.forEach ? src.length : Object.keys(src).length;

			Each( src, (key, rec) => {
				cb( rec, key, msg => {
					if ( msg != undefined ) 
						msgs.push( msg );
						/*
						if ( A.forEach ) 
							msgs.push( msg );
						else
							msgs[key] = msg;
						*/

					if ( ++returns == calls ) // signal end
						cb( null, msgs );
				});
			});

			if ( !calls ) cb( null, msgs );
		}
	},

	Clock: Clock,

	// Fetch options

	defHost: ENV.SERVICE_MASTER_URL,
	maxFiles: 1000,						//< max files to index
	maxRetry: 5,		// fetch wget/curl maxRetry	
	certs: { 		// data fetching certs
		pfx: FS.readFileSync(`./certs/fetch.pfx`),
		key: FS.readFileSync(`./certs/fetch.key`),
		crt: FS.readFileSync(`./certs/fetch.crt`),
		ca: "", //FS.readFileSync(`./certs/fetch.ca`),			
		_pfx: `./certs/fetch.pfx`,
		_crt: `./certs/fetch.crt`,
		_key: `./certs/fetch.key`,
		_ca: `./certs/fetch.ca`,
		_pass: ENV.FETCH_PASS
	},
	oauthHosts: {	// auth 2.0 hosts
	},

/**
Fetches text from a url of the form

	PROTOCOL://HOST/FILE ? batch=N & limit=N & rekey=from:to,... & comma=X & newline=X 

using a PUT || POST || DELETE || GET corresponding to the type of the fetch `data`  
Array || Object || null || Function where

	PROTOCOL		uses
	==============================================
	http(s) 		http (https) protocol
	curl(s) 		curl (presents certs/fetch.pfx certificate to endpoint)
	wget(s)			wget (presents certs/fetch.pfx certificate to endpoint)
	mask 			rotated proxies
	file			file system
	book			selected notebook record
	AASRV 			oauth authorization-authentication protocol (e.g. lexis)

While the FILE spec is terminated by a "/", a folder index is returned.  The optional 
batch,limit,... query parameters are used to regulate a (e.g. csv) file stream.

@extends String
@param {String} path source URL
@param {string | array | function | null} data fetching data or callback 
@param {TSR} [cb] callback when specified data is not a Function

@example
URL.fetchFile( text => {			// get request
})

@example
URL.fetchFile( [ ... ], stat => { 	// post request with data hash list
})

@example
URL.fetchFile( { ... }, stat => { 	// put request with data hash
})

@example
URL.fetchFile( null, stat => {		// delete request 
})

*/
	Fetch: (url, data, cb) => {	//< data fetcher

		function sha256(s) { // reserved for other functionality
			return CRYPTO.createHash('sha256').update(s).digest('base64');
		}

		function request(proto, opts, data, cb) {
			//Log(">>>req opts", opts);

			const Req = proto.request(opts, Res => { // get reponse body text
				var body = "";
				Res.on("data", chunk => body += chunk.toString() );

				Res.on("end", () => {
					//Log('fetch statusCode:', Res.statusCode);
					//Log('fetch headers:', Res.headers['public-key-pins']);	// Print the HPKP values

					if ( cb ) 
						cb( body );

					else
						data( body );
				});
			});

			Req.on('error', err => {
				Log(">>>fetch req", err);
				(cb||data)("");
			});

			switch (opts.method) {
				case "DELETE":
				case "GET": 
					break;

				case "POST":
				case "PUT":
					//Log(">>>post", data);
					Req.write( data ); //JSON.stringify(data) );  // post parms
					break;
			}					

			Req.end();
		}

		function agentRequest(proto, opts, sql, id, cb) {
			var 
				body = "",
				req = proto.get( opts, res => {
					var sink = new STREAM.Writable({
						objectMode: true,
						write: (buf,en,sinkcb) => {
							body += buf;
							sinkcb(null);  // signal no errors
						}
					});

					sink
					.on("finish", () => {
						var stat = "s"+Math.trunc(res.statusCode/100)+"xx";
						Log(">>>>fetch body", body.length, ">>stat",res.statusCode,">>>",stat);

						sql.query("UPDATE openv.proxies SET hits=hits+1, ?? = ?? + 1 WHERE ?", [stat,stat,id] );

						cb( (stat = "s2xx") ? body : "" );
					})
					.on("error", err => {
						Log(">>>fetch get", err);
						cb("");
					});

					res.pipe(sink);
				});

			req.on("socket", sock => {
				sock.setTimeout(2e3, () => {
					req.abort();
					Log(">>>fetch timeout");
					sql.query("UPDATE openv.proxies SET hits=hits+1, sTimeout = sTimeout+1 WHERE ?", id);
				});

				sock.on("error", err => {
					req.abort();
					Log(">>>fetch refused");
					sql.query("UPDATE openv.proxies SET hits=hits+1, sRefused = sRefused+1 WHERE ?", id);
				});
			});

			req.on("error", err => {
				Log(">>>fetch abort",err);
				sql.query("UPDATE openv.proxies SET hits=hits+1, sAbort = sAbort+1 WHERE ?", id);
			});
		}

		function getFile(path, cb) {
			const
				src = "."+path;

			if ( path.endsWith("/") )  // index requested folder
				try {
					const 
						files = [];

					//Log(">>index", src;
					FS.readdirSync( src).forEach( file => {
						var
							ignore = file.startsWith(".") || file.startsWith("~") || file.startsWith("_") || file.startsWith(".");

						if ( !ignore && files.length < maxFiles ) 
							files.push( (file.indexOf(".")>=0) ? file : file+"/" );
					});

					cb( files );
				}

				catch (err) {
					//Log(">>>fetch index error", err);
					cb( [] );
				}

			else 	// requesting static file
				try {		// these files are static so we never cache them
					FS.readFile(src, (err,buf) => res( err ? "" : Buffer.from(buf) ) );
				}

				catch (err) {
					Log(err);
					cb( null );
				};
		}	

		const
			{defHost,certs,maxRetry,oauthHosts,maxFiles} = ENUMS,
			opts = url.parseURL({}, defHost), 
			crud = {
				"Function": "GET",
				"Array": "PUT",
				"Object": "POST",
				"Null": "DELETE"
			},

			// for wget-curl
			cert = certs.fetch,
			wget = url.split("////"),
			wurl = wget[0],
			wout = wget[1] || "./temps/wget.jpg",

			// OAuth 2.0 host
			oauth = oauthHosts[opts.protocol],

			// response callback
			res = cb || data || (res => {}),
			method = crud[ data ? typeOf(data) : "Null" ] ;

		// opts.cipher = " ... "
		// opts.headers = { ... }
		// opts.Cookie = ["x=y", ...]
		// opts.port = opts.port ||  (protocol.endsWith("s:") ? 443 : 80);
		/*
		if (opts.soap) {
			opts.headers = {
				"Content-Type": "application/soap+xml; charset=utf-8",
				"Content-Length": opts.soap.length
			};
			opts.method = "POST";
		}*/

		//Log("FETCH",path);

		opts.method = method;

		switch ( opts.protocol ) {
			case "curl:": 
				CP.exec( `curl --retry ${maxRetry} ` + path.replace(opts.protocol, "http:"), (err,out) => {
					res( err ? "" : out );
				});
				break;

			case "curls:":
				CP.exec( `curl --retry ${maxRetry} -gk --cert ${cert._crt}:${cert._pass} --key ${cert._key} --cacert ${cert._ca}` + url.replace(protocol, "https:"), (err,out) => {
					res( err ? "" : out );
				});	
				break;

			case "wget:":
				CP.exec( `wget --tries=${maxRetry} -O ${wout} ` + path.replace(opts.protocol, "http:"), err => {
					res( err ? "" : "ok" );
				});
				break;

			case "wgets:":
				CP.exec( `wget --tries=${maxRetry} -O ${wout} --no-check-certificate --certificate ${cert._crt} --private-key ${cert._key} ` + wurl.replace(protocol, "https:"), err => {
					res( err ? "" : "ok" );
				});
				break;

			case "https:":
				/*
				// experiment pinning tests
				opts.checkServerIdentity = function(host, cert) {
					// Make sure the certificate is issued to the host we are connected to
					const err = TLS.checkServerIdentity(host, cert);
					if (err) {
						Log("tls error", err);
						return err;
					}

					// Pin the public key, similar to HPKP pin-sha25 pinning
					const pubkey256 = 'pL1+qb9HTMRZJmuC/bB/ZI9d302BYrrqiVuRyW+DGrU=';
					if (sha256(cert.pubkey) !== pubkey256) {
						const msg = 'Certificate verification error: ' + `The public key of '${cert.subject.CN}' ` + 'does not match our pinned fingerprint';
						return new Error(msg);
					}

					// Pin the exact certificate, rather then the pub key
					const cert256 = '25:FE:39:32:D9:63:8C:8A:FC:A1:9A:29:87:' + 'D8:3E:4C:1D:98:JSDB:71:E4:1A:48:03:98:EA:22:6A:BD:8B:93:16';
					if (cert.fingerprint256 !== cert256) {
						const msg = 'Certificate verification error: ' +
						`The certificate of '${cert.subject.CN}' ` +
						'does not match our pinned fingerprint';
						return new Error(msg);
					}

					// This loop is informational only.
					// Print the certificate and public key fingerprints of all certs in the
					// chain. Its common to pin the public key of the issuer on the public
					// internet, while pinning the public key of the service in sensitive
					// environments.
					do {
						console.log('Subject Common Name:', cert.subject.CN);
						console.log('  Certificate SHA256 fingerprint:', cert.fingerprint256);

						hash = crypto.createHash('sha256');
						console.log('  Public key ping-sha256:', sha256(cert.pubkey));

						lastprint256 = cert.fingerprint256;
						cert = cert.issuerCertificate;
					} while (cert.fingerprint256 !== lastprint256);

					};
				*/
				/*
				opts.agent = new HTTPS.Agent( false 
					? {
							//pfx: cert.pfx,	// pfx or use cert-and-key
							cert: cert.crt,
							key: cert.key,
							passphrase: cert._pass
						} 
					: {
						} );
					*/
				opts.rejectUnauthorized = false;
				request(HTTPS, opts, data, cb);
				break;

			case "http:":
				//Log(opts);

				request(HTTP, opts, data, cb);
				break;

			case "mask:":
			case "mttp:":	// request via rotating proxies
				opts.protocol = "http:";
				sqlThread( sql => {
					sql.query(
						"SELECT ID,ip,port FROM openv.proxies WHERE ? ORDER BY rand() LIMIT 1",
						[{proto: "no"}], (err,recs) => {

						if ( rec = recs[0] ) {
							opts.agent = new AGENT( `http://${rec.ip}:${rec.port}` );
							Log(">>>agent",rec);
							agentRequest(HTTP, opts, sql, {ID:rec.ID}, res );
						}
					});
				});
				break;

			case "nb:":
			case "book:":
				const
					book = opts.host,
					name = opts.path.substr(1);

				sqlThread( sql => {
					sql.query( name
						? "SELECT * FROM app.? WHERE Name=?"
						: "SELECT * FROM app.?", 

						[ book, name ], 

						(err,recs) => cb( err ? "" : JSON.stringify(recs) ) );
				});
				break;

			case "file:":	// requesting file or folder index
				//Log("index file", [path], opts);
				getFile( opts.path.substr(1) ? opts.path : "/db/home/" , res );  
				break;

			default:	
				if ( oauth ) 	// request made via oauth 
					request(HTTPS, oauth.token, oauth.grant, token => {		// request access token
						//Log("token", token);
						try {
							const 
								Token = JSON.parse(token);

							opts.protocol = "https:";
							opts.headers = {
								Authorization: Token.token_type + " " + Token.access_token,
								Accept: "application/json;odata.metadata=minimal",
								Host: "services-api.lexisnexis.com",
								Connection: "Keep-Alive",
								"Content-Type": "application/json"
							};
							delete opts.auth;

							Log("token request", opts );
							request(HTTPS, opts, search => {	// request a document search
								if ( docopts = oauth.doc ) 	// get associated document
									try {	
										const
											Search = JSON.parse(search),
											rec = Search.value[0] || {},
											doclink = rec['DocumentContent@odata.mediaReadLink'];

										//Log( Object.keys(Search) );
										if ( doclink ) {
											if (1)
												Log("get doc", {
													doclink: doclink , 
													href: oauth.doc.href, 
													reckeys: Object.keys(rec), 
													ov: rec.Overview, 
													d: rec.Date
												});

											if ( docopts.length ) // string so do fetch
												Fetch( docopts + doclink, doc => {
													res(doc);
												});

											else {
												docopts.path += doclink;
												docopts.headers = opts.headers;
												docopts.method = "GET";
												Log("doc request", docopts);
												request( HTTPS, docopts, doc => {
													res(doc);
												});
											}
										}
									}

									catch (err) {
										Log(">>>fetch lexis bad search",err);
									}

								else
									res( search );
							});
						}

						catch (err) {
							Log(">>>fetch lexis bad token", token);
							res(null);
						}
					});

				else
					res( "" );
		}
	}

};

[
	function now() {
		return new Date( this.next );
	},
	
/**
Return the wait time to next event, with callback(wait,next) when at snapshot events.

Example below for ON = 4 and OFF = 3 steps of length vlock.every.

Here S|B|* indicates the end of snapshot|batch|start events.  The clock starts on epoch = OFF 
with a wait = 0.  The clock's host has 1 step to complete its batch tasks, and OFF steps to 
complete its snapshot tasks.  Here, the work CYCLE = ON+OFF with a utilization = ON/CYCLE.  
Use OFF = 0 to create a continious process.  

		S			*	B	B	B	S			*	B	B	B
		|			|	|	|	|	|			|	|	|	|		...
		|			|	|	|	|	|			|	|	|	|		
		x-->x-->x-->x-->x-->x-->x-->x-->x-->x-->x-->x-->x-->x-->x-->x-->x-->x
epoch	0	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17

		|<-- OFF -->|<---- ON ----->|
*/
	
	function tick(cb) {		// advance clock, return waitTime, and callback cb(nextTime) on state-change epochs 
		
		const
			{ max,trunc } = Math,
			{ step,every,on,off,epoch,trace,next,start,cycle } = this,
			now = new Date(),
			wait = max( 1e3, next.getTime() - now.getTime() );

		if (trace)
			Log( `e=${epoch} s=${step} w=${trunc(wait)} n=${next}` );

		if ( cycle > 1 )	// using on-off process
			if ( epoch % cycle == 0 ) {
				this.step = off;
				if (trace)
					Log( `e=${epoch} s=${step} w=${trunc(wait)} n=${next}` );
				if (cb) cb( wait, next );
			}
		
			else			// continious process
				this.step = 1;
		
		this.epoch += this.step;
		
		switch ( every ) {		// advance next job epoch
			case "yr":
			case "year": 	next.setFullYear( next.getFullYear() + this.step ); break;
			case "wk":
			case "week": 	next.setDate( next.getDate() + 7*this.step ); break;
			case "day": 	next.setDate( next.getDate() + this.step ); break;
			case "hr":
			case "hour": 	next.setHours( next.getHours() + this.step ); break;
			case "min":
			case "minute": 	next.setMinutes( next.getMinutes() + this.step ); break;
			case "sec":
			case "second": 	next.setSeconds( next.getSeconds() + this.step ); break;

			case "monday": 
			case "tuesday":
			case "wednesday":
			case "thursday":
			case "friday":
			case "saturday":
			case "sunday":
			case "mon":
			case "tue":
			case "wed":
			case "thr":
			case "fri":
			case "sat":
			case "sun":
				next.setDate( next.getDate() + 7 - next.getDay() + 1); 
				this.every = "week";	// now switch to weeks
				// Log(">>>>>clock next", this.next, this.every);
				break;
				
			default:
				next.setTime( next.getTime() + every*1e3 );

		}

		return wait;
	}
].Extend(Clock);

[	
/**
Serialize this Array to the callback cb(rec,info) or cb(null,stack) at end given 
a sync/async fetcher( rec, res ).
*/
	function serialize(fetcher, cb) {
		Stream( this, {}, (rec, key, res) => {
			if ( res )
				fetcher( rec, info => {
					cb(rec, info);	// forward results
					res();	// signal record processed w/o stacking any results
				});	
			
			else 
				cb( null, res );
		});
	},
	
	function any( cb ) {
		var test = false;
		if ( cb )
			if ( typeof cb == "function" )
				this.forEach( val => {
					if ( cb(val) ) test = true;
				});
		
			else
				this.forEach( val => {
					if ( val == cb ) test = true;
				});
				
		else
			this.forEach( val => {
				if ( val ) test = true;
			});
			
		return test;
	},
	
	function all( cb ) {
		var test = true;
		if ( cb )
			if ( typeof cb == "function" )
				this.forEach( val => {
					if ( !cb(val) ) test = false;
				});
		
			else
				this.forEach( val => {
					if ( val != cb ) test = false;
				});
				
		else
			this.forEach( val => {
				if ( !val ) test = false;
			});
			
		return test;
	},
	
/**
Get elements or records using a keyList "OPTION, ..." || "[KEY,...]" || "!shuffle" where 

	OPTION = [ SRCKEY || !rem || REGEXP || (JS) ] => [ TARKEY || !test ] || INDEX 
	REGEXP is any regular expression to match source record keys, 
	JS evaluates the script in the source record context, 
	INDEX is a record index to return, 
	!rem matches all remaining source keys, 
	!test accepts the record if its SRC is true, 
	!shuffle is a random permutation.

Pass a stash to store indexing variables.  Add an optional $ctx to the stash to define functions and variables
available to JS scripts.

@example

	[{a1:10,a2:0,b:20,c1:4,c2:"there"},{a1:11,a2:111,b:21,c1:5,c2:"hello"}].get("1,0,0", {} )
	>> [
	  { a1: 11, a2: 111, b: 21, c1: 5, c2: 'hello' },
	  { a1: 10, a2: 0, b: 20, c1: 4, c2: 'there' },
	  { a1: 10, a2: 0, b: 20, c1: 4, c2: 'there' }
	  ]

@example

	[{a1:10,a2:0,b:20,c1:4,c2:"there"},{a1:11,a2:111,b:21,c1:5,c2:"hello"}].get("b=>B,(a1+a2-10)=>!test,(a1+a2)=>C,!rem=>D", {} )
	>> [
	  {
		B: 21,
		'!test': 112,
		C: 122,
		D: { a1: 11, a2: 111, b: 21, c1: 5, c2: 'hello' }
	  }
	]

@example

	[{a1:10,a2:0,b:20,c1:4,c2:"there"},{a1:11,a2:111,b:21,c1:5,c2:"hello"}].get("[a1,a2]")
	>> [ [ 10, 0 ], [ 11, 111 ] ]

	[{a1:10,a2:0,b:20,c1:4,c2:"there"},{a1:11,a2:111,b:21,c1:5,c2:"hello"}].get("[a1]")
	>> [ [ 10 ], [ 11 ] ]

	[[1,2,3,4],[5,6,7,8]].get("[0]")
	>> [ [ 1 ], [ 5 ] ]
*/
	function get(keyList, stash) {
		
		const
			recs = this,
			rec0 = recs[0] || {},
			Recs = [],
			stashKeys = Object.keys(stash);
		var
			{ $index,$ctx } = stash;
		
		if ( keyList ) 
			if ( keyList.charAt(0) == "[" ) {
				recs.forEach ( rec => {
					var Rec = [];
					Recs.push( Rec );
					keyList.substr(1,keyList.length-2).split(",").forEach( key => Rec.push( rec[key] ) );					
				});
				return Recs;
			}
		
			else
				switch (keyList) {
					case "!shuffle":
						return Recs;
						break;

					default: 
						if ( ! $index ) {
							$index = stash["$index"] = [];

							keyList.split(",").forEach( key => {
								var idx = parseInt(key);

								//Log(key,idx,idx !== NaN);
								if ( isNaN(idx) ) {
									const 
										[srcKey,tarKey0] = key.split("=>"),
										tarKey = tarKey0 || srcKey;

									if ( tarKey in stash ) {
									}

									else
									switch (srcKey) {
										case "!rem":
										case "!":
											for ( var SrcKey in rec0 ) {
												var
													inside = false;
												const 
													tar = stash[tarKey] || (stash[tarKey] = {});

												Each( stash, (tarKey, keys) => {
													if ( keys.any )
														if ( keys.any( key => key == SrcKey ) ) inside = true;

													else
													if ( keys == SrcKey ) inside = true;
												});
												if (!inside) tar[SrcKey] = SrcKey;
												//console.log(inside, stash, SrcKey, stash);
											}
											break;

										default:
											if ( srcKey.indexOf( "(" ) >= 0 ) 
												stash[tarKey] = ctx => {
													try {
														return VM.runInContext( srcKey.replace(/;/g,","), VM.createContext(Copy($ctx||{},ctx)));
													}
													catch (err) {
														return null;
													}						
												};

											else
											if ( srcKey in rec0 )
												stash[tarKey] = srcKey;

											else {
												const
													findKey = new RegExp(srcKey),
													srcKeys = stash[tarKey] = [];

												for ( var key in rec0 )
													if ( key ) 
														if ( key.match(findKey) )
															srcKeys.push( key );

												if ( srcKeys.length == 1 ) stash[tarKey] = srcKeys[0];
											}
									}
								}

								else
									$index.push( idx );
							});	
						}

						if ( $index.length ) 
							$index.forEach( idx => Recs.push( new Object( recs[idx] ) ));

						else
							recs.forEach( rec => {
								const
									Rec = {};
								var
									keep = true;

								for ( tarKey in stash ) if ( !tarKey.startsWith("$") ) {
									const
										srcKeys = stash[tarKey];
									var
										save;

									if ( srcKeys.forEach ) {
										save = Rec[tarKey] = [];
										srcKeys.forEach( srcKey => save.push( rec[ srcKey ] ) );	
									}

									else
									if ( typeof srcKeys == "function" ) 
										save = Rec[tarKey] = srcKeys( rec );

									else
									if ( srcKeys.length )
										save = Rec[tarKey]= rec[srcKeys];

									else {
										save = Rec[tarKey] = {};
										Each( srcKeys, (TarKey,SrcKey) => save[TarKey] = rec[SrcKey] );
									}

									switch (tarKey) {
										case "!test":
										case "!":
											if ( save.forEach ) {
												keep = save.all( v => v?true:false );
												delete Rec[tarKey];
											}

											else
												keep = save?true:false;

											break;

										default:
											Rec[tarKey] = save;
									}

								}

								if ( keep ) Recs.push( Rec );
							});

						return Recs;
				}
		
		else
			return recs;
	},
	
	function put( cb ) {
		const tar = this;
		this.forEach( (arg,i) => tar[i] = cb(arg,i) );
		return this;
	},
	
	function select(cb) {
		const rtn = [];
		this.forEach( (arg,i) => {
			if ( res = cb(arg,i) ) rtn.push(res);
		});
		return rtn;
	}
	
].Extend(Array);

[
	function replaceSync( pat, cb ) {
		var todo = 0, done=0, rtn = this+"";
		
		rtn = this.replace( pat, (...args) => {
			const hold = `@@hold${todo++}`;
			
			//console.log("hold", hold, todo, done);
			cb( args, res => {
				//console.log("res return(", rtn, ")=>", res, done, todo);
				rtn = rtn.replace( hold, res );
				
				if ( ++done == todo ) 
					cb( rtn ); 
			});
			return hold;
		});
			
		//console.log("rtn scanned", rtn);
		if ( !todo ) cb( rtn );
	},
	
/**
Tag url with specified attributes.

@memberof String
@param {String} el tag html element or one of "?&/:=" 
@param {String} at tag attributes = {key: val, ...}
@return {String} tagged results
*/ 
	function tag(el,at) {
		switch (el) {
			case "/":
			case "?":
			case "&":   // tag a url
				var rtn = this;

				Each(at, (key,val) => {
					if ( val ) {
						rtn += el + key + "=" + val;
						el = "&";
					}
				});

				return rtn;	

			case "[]":
			case "()":
				var rtn = this+el.substr(0,1), sep="";
				Each(at, (key,val) => {
					rtn += sep + key + ":" + JSON.stringify(val);
					sep = ",";
				});
				return rtn+el.substr(-1);
				
			case ":":
			case "=":
				var rtn = this, sep="";
				Each(at, (key,val) => {
					rtn += sep + key + el + JSON.stringify(val);
					sep = ",";
				});
				return rtn;

			case "":
				return `<a href="${el}">${this}</a>`;

			default: // tag html

				var rtn = "<"+el+" ";

				if ( at )
					Each( at, (key,val) => {
						if ( val )
							rtn += key + "='" + val + "' ";
					});

				switch (el) {
					case "embed":
					case "img":
					case "link":
					case "input":
						return rtn+">" + this;
					default:
						return rtn+">" + this + "</"+el+">";
				}
		}
	},

/**
Parse "$.KEY" || "$[INDEX]" expressions given $ hash.

@memberof String
@param {Object} $ source hash
*/
	function parseEval($) {
		try {
			return eval(this+"");
		}
		
		catch (err) {
			return err+"";
		}
	},
	
/**
Run JS against string in specified context.

@memberof String
@param {Object} ctx context hash
*/
	function parseJS(ctx, cb) {
		try {
			return VM.runInContext( this+"", VM.createContext(ctx || {}));
		}
		catch (err) {
			//Log("parseJS", this+"", err, ctx);
			if ( cb ) 
				return cb(err);
			
			else
				return null;
		}
	},
	
/**
Return an EMAC "...${...}..." string using supplied context.

@memberof String
@param {Object} query context hash
*/
	function parse$(ctx) {
		try {
			return VM.runInContext( "`" + this + "`" , VM.createContext(ctx));
		}
		catch (err) {
			return err+"";
		}
	},
	
/**
Parse string into json or set to default value/callback if invalid json.

@memberof String
@param {Function | Object} def default object or callback that returns default
*/
	function parseJSON(def) {
		try { 
			return JSON.parse(this);
		}
		catch (err) {  
			//Log("jparse", this, err);
			return def ? (isFunction(def) ? def(this+"") : def) || null : null;
		}
	},

/**
Parse a "PATH?PARM&PARM&..." url into the specified query, index, flags, or keys hash
as directed by the PARM = ASKEY := REL || REL || _FLAG = VALUE where 
REL = X OP X || X, X = KEY || KEY$[IDX] || KEY$.KEY and returns [path,file,type].

@memberof String
@param {Object} query hash of query keys
@param {Object} index hash of sql-ized indexing keys
@param {Object} flags hash of flag keys
@param {Object} where hash of sql-ized conditional keys
*/
	function parsePath(query,index,flags,where) { 
		var 
			search = this+"",
			ops = {
				equal: /(.*?)(:=|<=|>=|\!=|_=|\!bin=|\!exp=|\!nlp=)(.*)/,
				other: /(.*?)(<|>|=)(.*)/ 
			},
			[xp, path, search] = search.match(/(.*?)\?(.*)/) || ["",search,""],
			[xf, area, table, type] = path.match( /\/(.*?)\/(.*)\.(.*)/ ) || path.match( /\/(.*?)\/(.*)/ ) || path.match( /(.*)\/(.*)\.(.*)/ ) || path.match( /(.*)\/(.*)(.*)/ ) || ["","","",""];
			
		
		//Log(">>>>path", [search, path, search, area, table, type]);
		
		operators.forEach( key => where[key] = {} );
		
		search.split("&").forEach( parm => {
			if (parm) {
				const 
					[lhs,op,rhs] = parm.parseOP(ops.equal, parm => parm.parseOP( ops.other, parm => [parm,".",""] )),
					RHS = rhs.parseJSON( arg => arg ); 
				
				// Log(">>>search", [parm,lhs,rhs,op]);
				
				if (lhs)
				switch (op) {
					case "=":
						switch ( lead = lhs.charAt(0) ) {
							case "_":
								flags[lhs.substr(1)] = RHS;
								break;
								
							default:
								where["="][lhs] = rhs;
								query[lhs] = RHS;
						}
						break;

					case "_":
						flags[lhs] = RHS;
						break;
						
					case ".":
						index[lhs] = "";
						break;
						
					case ":=":
						index[lhs] = rhs;
						break;
						
					case "<":
					case ">":
					case "<=":
					case ">=":
					case "!=":
					case "!bin=":
					case "!exp=":
					case "!nlp=":
						where[op][lhs] = rhs;
						break;
				}
			}
		});
		
		return [path,table,type,area,search];
	},

	function parseURL(opts,base) {
		const 
			url = this+"",
			{username,password,hostname,protocol,pathname,search,port,href} = new URL(url,base);

		//Log(">>>>url", URL(url,base) );

		return Copy( opts || {}, {
			auth: username + ":" + password,
			path: pathname + search,
			protocol: protocol,
			host: hostname,
			port: port,
			href: href
		});
	},

/**
Chunk stream at path by splitting into newline-terminated records.
Callback cb(record) until the limit is reached (until eof when !limit)
with cb(null) at end.

@memberof String
@param {String} path source file
@param {Object} opts {newline,limit} options
@param {Function) cb Callback(record)
*/
	function chunkFile({newline,limit},cb) {
		const
			path = this+"";

		FS.open( path, "r", (err, fd) => {
			Log(">>>chunk", path, err, limit);
			
			if (err) 
				cb(null);	// signal pipe end

			else {	// start pipe stream
				var 
					pos = 0,
					rem = "",
					run = true,
					src = FS.createReadStream( "", { fd:fd, encoding: "utf8" }),
					sink = new STREAM.Writable({
						objectMode: true,
						write: (bufs,en,sinkcb) => {
							//Log( ">>>>chunk stream", limit,pos,!limit || pos<limit);
							if ( run ) {
								var 
									recs = (rem+bufs).split(newline);

								rem = recs.pop();

								if ( limit )	// chunk limited records
									recs.forEach( (rec,n) => {
										if ( rec ) 
											if ( pos<limit ) 
												cb(rec,pos++);
											else {
												//Log(">>>>chunk halting");
												run = false;
											}
										else
											pos++;
									});
								
								else	// chunk all records
									recs.forEach( (rec,n) => {
										if ( rec ) 
											cb(rec,pos++);
										else
											pos++;
									});									
							}
							sinkcb(null);  // signal no errors
						}
					});

				sink
					.on("finish", () => {	// signal complete
						//Log(">>>chunk finish", limit, pos);
						if (!limit || pos<limit)	// flush buffer
							if ( rem ) cb(rem,pos);
					
						cb(null);	// signal complete
					})
					.on("error", err => cb(null) );

				src.pipe(sink);  // start the pipe
			}
		});
	},

/**
Split stream at path containing comma delimited values: when keys = [],
the record keys are determined by the first header record; when keys = 
[ 'key', 'key', ...], then header keys are preset; when keys = null, 
raw text records are returned; when keys = parse(buf) function, then this
function used to parse (e.g.) json records.  The file is chunked using the (newline,
limit) chinkFile parameters.  Callsback cb(record) for each record with
cb(null) at end.

@memberof String
@param {String} path source file
@param {Object} opts {keys,comma,newline,limit} options
@param {Function} cb Callback(record || null)
*/
	function splitFile({keys, comma, newline, limit}, cb) {
		const
			path = this+"",
			opts = {newline:newline,limit:limit};
	
		var 
			pos = 0;
		
		if ( keys ) {		// split csv/json file
			const
				parse = keys.forEach 
					? (buf,keys) => {		// parse csv/txt buffer
						if ( keys.length ) {	/// at data row
							var 
								rec = {},
								cols = buf.split(comma);

							keys.forEach( (key,m) => rec[key] = cols[m] );
							return rec;
						}

						else {	// at header row so define keys
							if ( buf.charCodeAt(0) > 255 ) buf=buf.substr(1);	// weird
							buf.split(",").forEach( key => keys.push(key) );
							//Log(">>>split header keys", keys);
							return null;
						}
					}

					: buf => keys( {} );	// parse file buffer

			path.chunkFile( opts, buf => {			// get a record buffer
				if ( buf ) 
					if (rec = parse(buf,keys) ) 	// have data record
						cb(rec,pos++);
					else							// have empty record
						pos++;

				else	// forward eof signal 
					cb(null);
			});
		}
		
		else				// split txt file
			path.chunkFile( opts, buf => {
				if ( buf ) 
					cb(buf,pos++);

				else	// forward end signal 
					cb(null);
			});			
	},

/**
Filter stream at path containing comma delimited values.  The file is split using the (keys,comma) 
file splitting parameters, and chunked using the (newline,comma) file chunking parameters. Callsback 
cb([record,...]) with a record batch (all records when !batch) with cb(null) at end.  
Use the optional rekey = "fromKey=>toKey, regexp=>toKey, (js)=>toKey, !rem=>toKey, fromKey=>!test ..." 
to rekey records.

@memberof String
@param {String} path source file
@param {Object} opts {keys,comma,newline,limit,rekey,batch} options
@param {Function} cb Callback( [record,...] || null )
*/
	function streamFile({batch, keys, comma, newline, limit, rekey}, cb) {
		const 
			path = this+"",
			recs = [],
			opts = {keys:keys, limit:limit, comma:comma||",", newline:newline||"\n"},
			rekeyStash = {
				$ctx: {
					$tag: (str,el,tags) => str.tag(el,tags),
					$link: (str,ref) => str.tag("a",{href:ref}),
					$grab: str => str.replace(/.*\#(.*)\#.*/, (str,arg) => arg )
					//+ $.get(list,"[from || regexp || (JS) || !rem] => [to || !test] || INDEX, ..."  )  
					//+ $.resize(256,256)
					//+ $.greyscale()
					//+ $.sym( {maps:{x:'RGB',y:'L'}, limits:{cols:N,rows:N}, levels:{x:N,y:N}} )					
				}
			};
		
		var
			pos = 0;
		
		path.splitFile( opts, rec => {
			if ( rec ) 
				if ( !batch || recs.length<batch ) 
					recs.push( rec );

				else {	// flush batch
					cb( recs.get(rekey,rekeyStash), pos+=recs.length );
					recs.length = 0;
				}
					
			else { // forward end signal
				//Log("!!!!!!!!!!!!eos", recs.length);
				cb(recs.get(rekey,rekeyStash), pos+=recs.length); // flush batch
				//Log("!!!!!!!!!!!!eos end");
				cb(null);	// signal end
			}
		});
	},
	
/**
Log message to console with optional request to place into syslogs
@memberof String
@param {String} msg message to trace
@param {Object} req request { sql, query, client, action, table } 
@param {Function} res response callback(msg)
*/
	function trace(msg,req,res) {	
		function cpu() {
			var sum = 0, util = OS.loadavg();
			for ( var n=0, N = util.length; n<N; n++) sum += util[0];
			return sum / N;
		}
		
		function mem() {
			return OS.freemem() / OS.totalmem();
		}
		
		const
			node = CLUSTER.isMaster ? 0 : CLUSTER.worker.id,
			log = node + ">"  + this.toUpperCase() + ">" + msg;
		
		(res||console.log)( log );
		
		if ( req ) {
			const { sql, query, client, action, table } = req;
			
			if ( sql ) 
				sql.query( "INSERT INTO openv.syslogs SET ? ", {
					Node: OS.hostname()+"."+node,
					Client: client,
					Table: table,
					At: new Date(),
					Case: query.Name || "",
					Action: action,
					Module: this,
					cpuUtil: cpu(),
					memUtil: mem(),
					Message: msg+""
				});
		}
	},
		
/**
Serialize this String to the callback(results) given a sync/asyn fetcher(rec,res) where
rec = {ID, arg0, arg1, ...} contains args produced by regex.  Provide a unique placeholder
key to back-substitute results.

@memberof String
@example

	"junkabc;junkdef;"
	.serialize( (rec,cb) => cb("$"), /junk([^;]*);/g, "@tag", msg => console.log(msg) )

produces:

	"$$"
*/
	function serialize( fetcher, regex, key, cb ) {  //< callback cb(str) after replacing regex using fetcher( rec, (ex) => "replace" ) and string place holder key
		var 
			recs = [],
			results = this.replace( regex, (arg0, arg1, arg2, arg3, arg4) => {  // put in place-holders
				recs.push( {ID: recs.length, arg0:arg0, arg1:arg1, arg2:arg2, arg3:arg3, arg4:arg4} );
				return key+(recs.length-1);
			});

		recs.serialize( fetcher, (rec,info) => {  // update place-holders with info 
			if (rec) 
				results = results.replace( key+rec.ID, str => info );

			else 
				cb( results );
		});

	},
	
	/*function parseOP(reg, failcb, workcb) {
		var 
			[x,lhs,op,rhs] = this.match(reg) || [];

		return op ? workcb ? workcb(lhs,op,rhs) : [lhs, op, rhs] : failcb ? failcb(this+"") : null;
	}  */

	function parseOP(reg, cb) {
		var 
			[x,lhs,op,rhs] = this.match(reg) || [];

		return op ? [lhs,op,rhs] : cb(this+"");
	}
].Extend(String);

Copy({
	oauthHosts: {
		"lex:": {		// lexis-nexis search
			grant: "grant_type=client_credentials",  // &scope=http://auth.lexisnexis.com/all
			token: "https://auth-api.lexisnexis.com/oauth/v2/token".parseURL({
				//rejectUnauthorized: false,
				method: "POST",
				auth: ENV.LEXISNEXIS,
				headers: {
					"Content-Type": "application/x-www-form-urlencoded"
				}
			})
		},
		"lexis:": {		// lexis-nexis search and get doc
			grant: "grant_type=client_credentials",  // &scope=http://auth.lexisnexis.com/all
			token: "https://auth-api.lexisnexis.com/oauth/v2/token".parseURL({
				//rejectUnauthorized: false,
				method: "POST",
				auth: ENV.LEXISNEXIS,
				headers: {
					"Content-Type": "application/x-www-form-urlencoded"
				}
			}),
			doc: "https://services-api.lexisnexis.com/v1/".parseURL()
			/*, {
				//rejectUnauthorized: false,
				method: "GET",
				auth: ENV.LEXISNEXIS,
				headers: {
					"Content-Type": "application/x-www-form-urlencoded"
				}
			}) */
				// "lex://services-api.lexisnexis.com/v1/"
		}
	}
}, ENUMS);


//================== Unit testing

async function LexisNexisTest(N,endpt,R,cb) {
	const start = new Date(), {random,trunc} = Math;
	var done = 0;
	Log(start);
	for ( var n=0; n<N; n++) 
		Fetch(endpt + (R?trunc(random()*R):""), res => {
			//Log(n,done,N);
			if ( ++done == N ) {
				var 
					stop = new Date(),
					mins = (stop-start)/1e3/60,
					rate = N/mins;
				
				Log(stop, mins, "mins", rate, "searches/min");
				
				if (cb) cb(res);
			}
		});
}

switch (process.argv[2]) {	//< unit testers
	case "?":
		Log("unit test with 'npm enum.js [E1 || ...]'");
		break;
		
	case "E1": 
		Log({
			shallowCopy: Copy( {a:1,b:2}, {} ),
			deepCopy: Copy({ a:1,"b.x":2 }, {b:{x:100}}, ".")
		});
		break;
		
	case "X":
		"testabc; testdef;".replaceSync(/test(.*?);/g, (args,cb) => {
			
			if ( cb ) {
				console.log("args", args);
				cb( "#"+args[1] );
			}
			
			else
				console.log("final", args);
		});
		
	
	case "LN1":
		LexisNexisTest(1e3, 'lex://services-api.lexisnexis.com/v1/News?$search=rudolph');
		break;
		
	case "LN2":
		LexisNexisTest(1e3, 'lex://services-api.lexisnexis.com/v1/News?$search=rudolph&$expand=Document');
		break;

	case "LN3":
		LexisNexisTest(1e3, 'lex://services-api.lexisnexis.com/v1/News?$search=rudolph');
		break;
			
	case "LN4":
		LexisNexisTest(1, 'lex://services-api.lexisnexis.com/v1/News?$search=rudolph', 0, res => {
			//Log("res=>", res);
			var r = JSON.parse(res);
			//Log( Object.keys(r) );
			if ( rec = r.value[0] ) 
				Log( "fetch docendpt >>>>", rec['DocumentContent@odata.mediaReadLink'] , Object.keys(rec), rec.Overview, rec.Date );
			
			Fetch( 'lex://services-api.lexisnexis.com/v1/' + rec['DocumentContent@odata.mediaReadLink'] , doc => {
				//    'lexis://services-api.lexisnexis.com/v1/MEDIALINK
				Log( "doc=>", doc );
			});
			
		});
		break;

	case "LN5":
		Fetch( 'lexis://services-api.lexisnexis.com/v1/News?$search=rudolph', doc => {
			Log( "doc=>", doc );
		});
		break;
		
	case "XX":
		"testabc; testdef;".replaceSync(/test(.*?);/g, (args,cb) => {
			//console.log(args);
			
			if ( cb ) 
				Fetch( "http://localhost:8080/nets.txt?x:=Name", txt => {
					//console.log("fetch", args, txt);
					cb( "#"+args[1] );
					//cb( "#"+txt );
				});
			
			else
				console.log("final", args); 
		});
		break;
}

// UNCLASSIFIED
