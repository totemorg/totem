// UNCLASSIFIED

/**
Provides mysql and neo4j agnosticators as well as a task queuer.

@module JSDB

@requires cluster
@requires mysql
@requires os
@requires neo4j-driver
@requires fs

@requires enums
*/

const	
	// nodejs modules
	ENV = process.env,
	CLUSTER = require("cluster"),
	OS = require("os"),	
	FS = require("fs"), 				//< filesystem and uploads

	// 3rd party bindings
	{ escape, escapeId } = MYSQL = require("mysql"),	// connector for mysql database
  	NEO4J = require("neo4j-driver"),					// connector the neo4j graph database	

	// totem
	{ Copy,Each,Stream,isFunction,isString,isArray,isEmpty,streamFile } = require("enums");

// jsdb i/f

const { Log, Trace, dropCard, wildMatch, sqlThread, neoThread } = JSDB = module.exports = {
	Log: (...args) => console.log(">>>jsdb", args),
	Trace: (msg,req,res) => "jsdb".trace(msg,req,res),

	/**
	Reserved for dataset attributes
	@cfg {Object}
	*/
	dsAttrs: {
	},
		
	/**
	@cfg {Object} 
	*/		

	config: (opts,cb) => {  
		if (opts) Copy(opts,JSDB,".");

		const
			{ mysqlOpts, neo4jOpts, attrs } = JSDB;
		
		// connect to mysql service
		
		/*	legacy method
			mysql.connection = MYSQL.createConnection(mysql.opts);
			mysql.connection.connect();
			// nonpooled connector permits connection reuse within other queries, but they become responsible to release() the connection
		*/

		var
			mysqlThreads = 0,
			sql = JSDB.mysqlPool = MYSQL.createPool( mysqlOpts )
			// reserved to test thread depth to protect against DOS attacks
			.on("acquire", () => mysqlThreads ++ )
			.on("release", () => mysqlThreads -- );
		
		[						
			// key getters
			getKeys,
			//getTypes,
			getFields,
			getTables,
			getJsons,
			getSearchables,
			getGeometries,
			getTexts,

			// query processing
			Format,
			Query,
			Index,
			Batch,
			Hawk,

			// record enumerators
			relock,
			forFirst,
			forEach,
			forAll,
			//thenDo,
			//onEnd,
			//onError,

			// misc
			//reroute,
			//serialize,
			context,
			cache,
			//flattenCatalog,

			/*
			//escapeing,
			function escape(arg) { return MYSQL.escape(arg); },
			function escapeId(key) { return MYSQL.escapeId(key); }, 
			*/

			// bulk insert records
			ingestFile,
			beginBulk,
			endBulk,

			// job processing
			queueTask,
			saveContext
		].Extend(sql.constructor);

		sql.query("select 123", (err,recs) => {
			if (cb) cb(err ? null : sql);
			
			if ( err ) {
				JSDB.mysqlPool = null;
				Trace("mysql service not running or login credentials are invalid");
				Log(mysqlOpts);
			}

			else {	// get attributes for jsdb datasets
				sql.query("DELETE FROM openv.locks");	// remove all record locks

				sql.query(`SHOW TABLES FROM app`, (err,recs) => {
					recs.forEach( rec => {
						sql.getSearchables( ds = "app." + rec.Tables_in_app, keys => {
							var attr = attrs[ds] = {};
							for (var key in attrs.default) attr[key] = attrs.default[key];
							attr.search = keys.join(",");
						});
					});
				});
			}
		});	
		
		// connect to neo4j service
		
		try {
			JSDB.neoDriver = NEO4J.driver( neo4jOpts.host, 
						NEO4J.auth.basic( neo4jOpts.user, neo4jOpts.password ), { 
				disableLosslessIntegers: true 
			} );
		}

		catch (err) {
			JSDB.neoDriver = null;
			Trace("neo4j service not running or login credentials are invalid");
			Log(neo4jOpts);
		};
	},
	
	/**
	@cfg {Object} 
	*/		

	savers: {
		_net: (path,nets) => {
			//Log(">>>save nets", paths);

			neoThread( neo => {
				nets.forEach( net => {
					const 
						{name, nodes, edges} = net;

					//neo.clearNet(netName);
					neo.saveNet( path+"_"+name, nodes, edges );
				});
			});
		},

		_jpg: (path,jpgs) => {
			jpgs.forEach( jpg => {
				const
					{ input, values, index } = jpg,
					img = input,
					cols = values.length,
					rows = index.length,
					isEmpty = values[0] ? false : true,
					toColor = JIMP.rgbaToInt;

				Log("save jpg", {
					dims: [img.bitmap.height, img.bitmap.width], 
					save: path,
					gen: [rows, cols],
					empty: isEmpty
				});

				if ( !isEmpty )
					for ( var col=0, vals=values[0]; col<cols; col++, vals=values[col] ) {
						//Log("vals", vals);
						for ( var row=0; row<rows; row++ ) {
							var L = max(0,min(255,floor(vals[row][0])));
							//Log(L, col, row, "->", index[row]);
							img.setPixelColor( toColor(L,L,L,255), col, index[row] );
						}
					}

				img.write( path, err => Log("save jpg", err) );

				delete jpg.input;

				/*
				if (keep) {
					jpg.values = stat.values.shuffle(keep); // Array.from(values.slice(0,keep)).$( (n,v) => v[n] = v[n].slice(0,keep) );
					jpg.index = index.shuffle(keep); // index.slice(0,keep);
				}
				else {
					delete jpg.values;
					delete jpg.index;
				}*/
			});
		},

		_txt: (path,txts) => {
			FS.writeFileSync( "./stores/"+path+".txt", txt.join("\n"), "utf8");
		},

		_json: (path,files) => { 
			Each(files, (key,data) => {
				//Log(">>>save file", key);
				FS.writeFileSync( "./stores/"+path+"_"+key+".json", JSON.stringify(data), "utf8");			
			});
		}
	},
		
	/**
	@cfg {Object} 
	*/		
	dropCard: "$drop",
	
	/**
	@cfg {Object} 
	*/		
	wildMatch: /\*/g,
	
	/**
	@cfg {Object} 
	*/		
	queues: { 	//< reserve for job queues
	},

	/**
	@cfg {Object} 
	*/		
	errors: {		//< errors messages
		noConnect: new Error("sql pool exhausted or undefined"),
		nillUpdate: new Error("nill update query"),
		unsafeQuery: new Error("unsafe queries not allowed"),
		unsupportedQuery: new Error("query not supported"),
		invalidQuery: new Error("query invalid"),
		noTable: new Error("dataset definition missing table name"),
		noDB: new Error("no database connected"),
		noLock: new Error("record lock ID missing"),
		isUnlocked: new Error("record never locked"),
		failLock: new Error("record locking failed"),
		isLocked: new Error("record already locked"),
		noExe: new Error("record execute undefined"),
		noRecord: new Error("no record found")
	},

	//probeSite: (url,opt) => { throw new Error("data probeSite not configured"); }, //< data probeSite

	/**
	@cfg {Object} 
	*/		
	attrs: {		//< reserved for dataset attributes derived during config
		default:	{ 					// default dataset attributes
			sql: null, // sql connector
			query: "",  // sql query
			opts: null,	// ?-options to sql query
			unsafeok: true,  // allow/disallow unsafe queries
			trace: false,   // trace ?-compressed sql queries
			journal: true,	// attempt journally of updates to jou.table database
			ag: "", 		// default aggregator "" implies "least(?,1)"
			index: {select:"*"}, 	// data search and index
			client: "guest", 		// default client 
			//track: false, 		// change journal tracking
			search: ""  // key,key, .... fulltext keys to search
		}		
	},

	/**
	MySQL connection options or null to disable
	@cfg {Object} 
	*/		
	mysqlOpts: { //< mysql options
		// login credentials
		host: ENV.MYSQL_HOST || "MYSQL_HOST undefined",
		user: ENV.MYSQL_USER || "MYSQL_USER undefined",
		password: ENV.MYSQL_PASS || "MYSQL_PASS undefined",
		port: ENV.MYSQL_PORT || 3306,
		
		// connection options
		connectionLimit: 50,	// max number to create "at once" - whatever that means
		acquireTimeout: 600e3,	// ms timeout during connection acquisition - whatever that means
		connectTimeout: 600e3,	// ms timeout during initial connect to mysql server
		queueLimit: 0,  						// max concections to queue (0=unlimited)
		waitForConnections: true,		// queue when no connections are available and limit reached
		
		
		// reserved for ...
		threads: 0, 	// connection threads
		pool: null		// connector
		/*
		function dummyConnector() {
			var
				This = this,
				err = JSDB.errors.noDB;

			this.query = function (q,args,cb) {
				Trace("NODB "+q);
				if (cb)
					cb(err);
				else
				if ( args && isFunction(args) )
					args(err);

				return This;
			};

			this.on = function (ev, cb) {
				return This;
			};

			this.sql = "DUMMY SQL CONNECTOR";

			this.release = function () {
				return This;
			};

			this.createPool = function (opts) {
				return null;
			};
		} */		
	},
	
	neo4jOpts: {	// neo4j options
		host: ENV.NEO4J_HOST || "NEO4J_HOST undefined", 
		user: ENV.NEO4J_USER || "NEO4J_USER undefined", 
		password: ENV.NEO4J_PASS || "NEO4J_PASS undefined"				
	},
			
	neoThread: cb => {
		cb( JSDB.neoDriver ? new NEOCON( false ) : null );
	},

	// callback cb(sql) with a sql connection
	sqlThread: cb => {
		const
			{ mysqlPool } = JSDB;

		cb( mysqlPool );
		
		/*
		if ( mysqlPool ) 
			mysqlPool.getConnection( (err,sql) => {
				if (err) 
					Log( JSDB.errors.noConnect, {
						error: err,
						total: mysqlPool._allConnections.length ,
						free: mysqlPool._freeConnections.length,
						queue: mysqlPool._connectionQueue.length,
						config: mysql
					});

				else {
					cb( sql );
					sql.release();
				}
			});

		else
		if ( sql = MYSQL.createConnection(mysql.opts) ) {
			cb( sql );
			//sql.release();
		}
		else
			Log(JSDB.errors.noConnect);	
		*/
	},

	sqlEach: (trace, query, args, cb) => sqlThread( sql => sql.forEach( trace, query, args, rec => cb(rec, sql) ) ),

	sqlAll: (trace, query, args, cb) => sqlThread( sql => sql.forAll( trace, query, args, recs => cb(recs, sql) ) ),

	sqlFirst: (trace, query, args, cb) => sqlThread( sql => sql.forFirst(trace, query, args, rec => cb(rec, sql) ) ),
	
	sqlContext: (ctx, cb) => sqlThread( sql => sql.context( ctx, dsctx => cb(dsctx, sql) ) )

	/*	
	thread: sqlThread,
	forEach: sqlEach,
	forAll: sqlAll,
	forFirst: sqlFirst,
	context: sqlContext 
	*/
};
	
//=================== neo4j 

function NEOCON(trace) {
	this.trace = trace || false;
}
	
[
	function cypher(query,params,cb) {// submit cypher query to neo4j

		var 
			neo = this,
			ses = JSDB.neoDriver.session();	// no pooled connectors so must create and close them

		if ( params )
			Each( params, (key,val) => {	// fix stupid $tokens in neo4j driver
				if (val) {
					if ( isObject(val) ) {
						query = query.replace(new RegExp("\\$"+key,"g"), arg => "{".tag(":",val)+"}" );
						delete params[key];
					}
				}
				
				else
					Log("???????????????????? neo", key,val, params);
			});

		// if ( neo.trace) Log(query);

		ses
		.run( query, params )
		.then( res => {
			
			var 
				recs = res.records,
				Recs = [];
			
			if (recs)
				recs.forEach( (rec,n) => {
					var Rec = {};
					rec.keys.forEach( key => Rec[key] = rec.get(key) );
					Recs.push( Rec );
				});
			
			if (cb) cb(null, Recs);
		})
		.catch( err => {
			if ( neo.trace) Log(err);
			//if (cb) cb( err, null );
		})
		.then( () => {
			ses.close();
		})
	},

	function clearNet( net ) {
		Log( "clear net", net);	
		
		this.cypher( `MATCH (n:${net}) DETACH DELETE n` );
	},
	
	function saveNodes(net, nodes, res ) {		// add typed-nodes to neo4j
		var 
			neo = this,
			trace = neo.trace;

		Stream( nodes, {}, (node, name, cb) => {
			if (cb) { // add node
				neo.cypher(
					`MERGE (n:${net}:${node.type} {name:$name}) ON CREATE SET n += $props`, {
						name: name,
						props: node
				}, err => {
					// if ( trace ) Log(">>>neo save node", err || "ok");
					cb();
				});
			}
					 
			else	// all nodes processed so move forward
				res( null );
		});
		
	},

	/*
	function makeEdge( net, edge ) { // link existing typed-nodes by topic
		var 
			[src,tar,props] = edge,
			neo = this;
		
		//Log("edge", src.name, tar.name, props);
		neo.cypher(
			`MATCH (a:${net} {name:$srcName}), (b:${net} {name:$tarName}) `
			+ "MERGE "
			+ `(a)-[r:${props.name}]-(b) `
			+ "ON CREATE SET r = $props ", {
					srcName: src.name,
					tarName: tar.name,
					props: props || {}
		}, err => {
			if (err) Log(">>>create edge failed", [src.name, tar.name] );
		});
	},*/
	
	function findAssoc( query, cb ) {
		const 
			neo = this,
			[src,tar,rel] = query,
			Rel = rel.replace( / /g, "");
		
		neoThread( neo => {
			neo.cypher( 
				`MATCH ( a {name:$src} ) -[r:${Rel}]-> ( b {name:$tar} ) RETURN r,a,b`, 
				{
					src: src,
					tar: tar
				}, 
				(err,recs) => {
				
				if (err) Log(err);
				cb( recs, query );
			});
		});		
	},
	
	function saveNet( net, nodes, edges ) {
		const 
			neo = this;
		
		//neo.cypher( `CREATE CONSTRAINT ON (n:${net}) ASSERT n.name IS UNIQUE` );

		Log(">>>neo save net", net);
		
		neo.saveNodes( net, nodes, () => {
			//Log(">> edges", edges, "db=", db);
			//Each( edges, (name,pairs) => neo.saveEdges( net, pairs ) );
			neo.saveEdges( net, edges ) ;
		});	
	},
	
	function saveEdges( net, edges ) {
		const 
			neo = this,
			trace = neo.trace;
		
		//Log("save pairs topic", name);
		Each( edges, (name,edge) => {
			//neo.makeEdge( net, edge );
			var Type = edge.type.replace(/ /g, "");
			
			neo.cypher(
				`MATCH (a:${net} {name:$src}), (b:${net} {name:$tar}) MERGE (a)-[r:${Type}]-(b) ON CREATE SET r = $props`, {
				src: edge.src,
				tar: edge.tar,
				props: edge || {}
			}, err => {
				//if ( trace ) 
				//Log(">>>neo save edge", err || "ok" );
			});
			
		});
	}

].Extend(NEOCON);

//============ mysql key access

function getKeys(table, query, keys, cb) {
	this.query(
		(where = whereify({"=":query}))
			? `SHOW KEYS FROM ?? WHERE least(${where},1)`
			: "SHOW KEYS FROM ?? ",
		
		table, (err,recs) => {
			
			if (!err)
				if ( keys.push )
					recs.forEach( rec => keys.push(rec.Column_name) );
			
				else
					recs.forEach( rec => keys[rec.Field] = rec.Index_type );
			
			cb(keys);
	});
}

function getFields(table, query, keys, cb) {
	this.query( 
		(where = whereify({"=":query}))
			? `SHOW FULL FIELDS FROM ?? WHERE least(${where},1)`
			: "SHOW FULL FIELDS FROM ?? ", 
		
		table, (err, recs) => {
			
			if (!err) 
				if ( keys.push )
					recs.forEach( (rec,n) => keys.push(rec.Field) );
					
				else
					recs.forEach( rec => keys[rec.Field] = rec.Type );
			
			cb(keys);
	});
}

function getJsons(table, cb) {
	this.getFields(table, {Type:"json"}, [], cb);
}

function getTexts(table, cb) {
	this.getFields(table, {Type:"mediumtext"}, [], cb);
}

function getSearchables(table, cb) {
	this.getKeys(table, {Index_type:"fulltext"}, [], cb);
}

function getGeometries(table, cb) {
	this.getFields(table, {Type:"geometry"}, [], cb);
}

function getTables(db, cb) {
	var 
		key = `Tables_in_${db}`,
		tables = [];
				  
	this.query( "SHOW TABLES FROM ??", [db], (err,recs) => {
		if ( !err ) {
			recs.forEach( rec => {
				//tables[ rec[key] ] = db;
				tables.push( rec[key] );
			});
			cb( tables );
		}
	});
}

function context(ctx,cb) {  // callback cb(dsctx) with a JSDB context
	var 
		sql = this,
		dsctx = {};
	
	Each(ctx, function (dskey, dsats) {
		dsctx[dskey] = new DATASET( sql, dsats );
	});
	cb(dsctx);
}

//============== Record cacheing and bulk record inserts
 
/**
	Implements generic cache.  Looks for cache given opts.key and, if found, returns cached results on cb(results);
	otherwse, if not found, returns results via opts.make(probeSite, opts.parms, cb).  If cacheing fails, then opts.default 
	is returned.  The returned results will always contain a results.ID for its cached ID.  If a opts.default is not provided,
	then the cb callback in not made.
*/
function cache( opts, cb ) {	// quasi legacy - geohack may use
	var 
		sql = this,
		//probeSite = JSDB.probeSite,
		defRec = {ID:0};
	
	if ( opts.key )
		sql.forFirst( 
			"", 
			"SELECT ID,Results FROM openv.cache WHERE least(?,1) LIMIT 1", 
			[ opts.key ], rec => {

			if (rec) 
				try {
					cb( Copy( JSON.parse(rec.Results), {ID:rec.ID}) );
				}
				catch (err) {
					if ( opts.default )
						cb( Copy(opts.default, defRec ) );
				}

			else
			if ( opts.make ) 
				if (probeSite)
					opts.make( probeSite.tag("?",opts.parms || {}), ctx => {

					if (ctx) 
						sql.query( 
							"INSERT INTO openv.cache SET Added=now(), Results=?, ?", 
							[ JSON.stringify(ctx || opts.default), opts.key ], 
							function (err, info) {
								cb( Copy(ctx, {ID: err ? 0 : info.insertId}) );
						});

					else 
					if ( opts.default )
						cb( Copy(opts.default, {ID: 0}) );
				});
				
				else
					cb( defRec );

			else
			if ( opts.default )
				cb( Copy(opts.default, defRec) );
		});
	
	else
	if ( opts.default )
		cb( Copy(opts.default, defRec) );
	
}

function beginBulk() {
	this.query("SET GLOBAL sync_binlog=0");
	this.query("SET GLOBAL innodb-flush-log-at-trx-commit=0");
	this.query("START TRANSACTION");
}

function endBulk() {
	this.query("COMMIT");
	this.query("SET GLOBAL sync_binlog=1");
	this.query("SET GLOBAL innodb-flush-log-at-trx-commit=1");
}

//================= catalog interface

/**
	@method flattenCatalog
	Flatten entire database for searching the catalog

	Need to rework using serialize
*/
function flattenCatalog(flags, catalog, limits, cb) {	// legacy but could be useful with rework
	
	function flatten( sql, rtns, depth, order, catalog, limits, cb) {
		var table = order[depth];
		
		if (table) {
			var match = catalog[table];
			var filter = cb.filter(match);
			
			var quality = " using "+ (filter ? filter : "open")  + " search limit " + limits.records;
			
			Trace("CATALOG "+table+quality+" RECS "+rtns.length, sql);
		
			var query = filter 
					? "SELECT SQL_CALC_FOUND_ROWS " + match + ",ID, " + filter + " FROM ?? HAVING Score>? LIMIT 0,?"
					: "SELECT SQL_CALC_FOUND_ROWS " + match + ",ID FROM ?? LIMIT 0,?";
					
			var args = filter
					? [table, limits.score, limits.records]
					: [table, limits.records];

			sql.query( query, args,  (err,recs) => {
				if (err) {
					rtns.push( {
						ID: rtns.length,
						Ref: table,
						Name: "error",
						Dated: limits.stamp,
						Searched: 0,
						Link: (table + ".db").link("/" + table + ".db"),
						Content: err+""
					} );

					flatten( sql, rtns, depth+1, order, catalog, limits, cb );
				}
				else 
					sql.query("select found_rows()")
					.on('result', function (stat) {
						recs.forEach( rec => {						
							rtns.push( {
								ID: rtns.length,
								Ref: table,
								Name: `${table}.${rec.ID}`,
								Dated: limits.stamp,
								Quality: recs.length + " of " + stat["found_rows()"] + quality,
								Link: table.link( "/" + table + ".db?ID=" + rec.ID),
								Content: JSON.stringify( rec )
							} );
						});

						flatten( sql, rtns, depth+1, order, catalog, limits, cb );
					});
			});	
		}
		else
			cb.res(rtns);
	}

	var 
		sql = this,
		rtns = [];
		/*limits = {
			records: 100,
			stamp: new Date()
			//pivots: flags._pivot || ""
		};*/
	
	// need to revise this to use serialize logic
	flatten( sql, rtns, 0, FLEX.listify(catalog), catalog, limits, {
		res: cb, 

		filter: function (search) {
			return ""; //Builds( "", search, flags);  //reserved for nlp, etc filters
	} });
}

//================= record enumerators

function forFirst(msg, query, args, cb) {  // callback cb(rec) or cb(null) on error
	var q = this.query( query || "#ignore", args, (err,recs) => {  
		if ( err ) 
			cb( null );
		
		else
			cb( recs[0] || null );
	});
	if (msg) Trace( `${msg} ${q.sql}`, this);	
	return q;
}

function forEach(msg, query, args, cb) { // callback cb(rec) with each rec - no cb if errror
	q = this.query( query || "#ignore", args).on("result", rec => cb(rec) );
	if (msg) Trace( `${msg} ${q.sql}`, this);	
	return q;
}

function forAll(msg, query, args, cb) { // callback cb(recs) of cb(null) on error
	var q = this.query( query || "#ignore", args, (err,recs) => {
		if ( err ) 
			cb( null );
		
		else
			cb( recs );
	});
	if (msg) Trace( `${msg} ${q.sql}`, this);	
	return q;
}

function Format(query,opts) {
	
	var 
		sql = this,
		{index,where,having,limit,offset,sort,group,set,pivot,browse} = opts || {},
		sorts = [];
	
	if ( pivot ) {
		var 
			where = where || {},
			slash = "_",
			nodeID = where.NodeID || "root",
			index = (nodeID == "root") 
				? {
					Node: pivot,
					ID: `group_concat(DISTINCT ID SEPARATOR '${slash}') AS ID`,
					Count: "count(ID) AS Count",
					leaf: "false AS leaf",
					expandable: "true AS expandable",
					expanded: "false AS expanded"
				}
				: {
					Node: pivot,
					ID: `'${nodeID} AS ID`,
					Count: "1 AS Count",
					leaf: "true AS leaf",
					expandable: "true AS expandable",
					expanded: "false AS expanded"
					//ID: `$${nodeID} AS ID`
				};

		if (nodeID == "root") {
			group = pivot;
			delete where.NodeID;
		}

		else 
			group = null;

		//Log( "jsdb piv", index);
	}

	else
	if ( browse ) {
		var	
			slash = "_", 
			where = where || {},
			nodeID = where.NodeID,
			nodes = nodeID ? nodeID.split(slash) : [],
			pivots = browse.split(","),
			group = (nodes.length >= pivots.length)
				? pivots.concat(["ID"])
				: pivots.slice(0,nodes.length+1),
			name = pivots[nodes.length] || "concat('ID',ID)",
			path = group.join(",'"+slash+"',"),
			index = {
				Node: browse,
				ID: `group_concat(DISTINCT ${path}) AS ID`,
				Count: "count(ID) AS Count",
				path: '/tbd AS path',
				read: "1 AS read",
				write: "1 AS write",
				group: "'v1' AS group",
				locked: "1 AS locked"
			};

		index[name+":"] = `cast(${name} AS char)`;

		delete where.NodeID;
		nodes.forEach( node => where[ pivots[n] || "ID" ] = node);
	}
	
	if ( sort )
		try {
			if ( sort.forEach )
				sort.forEach( opt => sorts.push( 
					escapeId(opt.property || opt) + " " + (opt.direction||"") ) );

			else 
				sorts = sort.split(",");
		}
		catch (err) {
		}
	
	//Log(">>>>",where,isEmpty(where));
	
	const 
		f = {
			where: whereify(where || {}),
			having: whereify(having || {})
		};
	
	return query.parse$({
		set: isEmpty(set) ? "() values ()" : "SET ?",
		index: index || "*",
		where: f.where ? `WHERE least(${f.where},1)` : "",
		having: f.having ? `HAVING least(${f.having},1)` : "",
		limit: limit ? "LIMIT "+limit : "",
		offset: offset ? "OFFSET "+offset : "",
		sort: sort ? "ORDER BY "+sorts.join(",") : "",
		group: group ? group.forEach ? group.join(",") : group : ""
	});
}

function Query(query,args,opts,cb) {
	const
		q = this.Format(query,opts),
		[cmd,calc] = q.toLowerCase().split(" "),
		{trace,set,client,where,lock,from,jsons,sio} = opts || {},
		sql = this;
	
	if ( true ) Log(q);
	
	if ( lock ) {
		sql.ctx = opts;
		sql.relock( () => {  // sucessfully unlocked
			switch ( cmd ) {
				case "select": break;
				case "delete": 	sql.ds = null; break;
				case "update":	sql.ds = set; break;
				case "insert":	sql.ds = [set]; break;
			}

			cb( sql.ctx.err, null );
		}, () => {  // sucessfully locked
			cb( sql.ctx.err, null );
			//res( rec );
		});
	}
	
	else
	if ( calc == "SQL_CALC_FOUND_ROWS" ) 
		sql.getConnection( (err, con) => {	// found_rows needs to be on same connection
			con.query( q, args, (err, recs) => {
				if ( !err )
					con.query("SELECT found_rows() AS Found", [], (err,info) => {
						recs.found = info[0].Found || recs.length;
						cb( err, recs || [] );
						con.release();
					});

				else {
					cb( err, recs || [] );
					con.release();
				}
			});
		});	
			
	else 
		sql.query( q, args, (err, info) => {
			if ( err ) 
				cb(err);
			
			else {
				if ( jsons && info.forEach ) 
					jsons.forEach( key => {
						try {
							info.forEach( rec => rec[key] = JSON.parse( rec[key] ) );
						}
						
						catch (err) {
						}
					});

				cb( err, info );

				if ( sio ) { // Notify other clients of change
					//Log(cmd, client, "=>", Object.keys(sio.clients) );
					sio.emit( cmd, {
						ds: from, 
						change: set, 
						recID: where["="].ID, 
						by: client
					});	
				}
			}
		});

}

function Index(ds, query, jsons, cb) {	// callback cb( "key,key as key,...", jsons)
	function getJSON( key, cb ) {
		var 
			get = "",
			keys = key.split("$");

		if ( keys.length > 1 ) {
			keys.forEach( (idx,n) => {
				if (n) {
					var keys = idx ? idx.split(",").join("','$") : "";
					
					get = `json_extract( ${get}, '$${keys}' )`;
				}

				else
					get = escapeId(idx);
			});
			cb( get );
		}
		
		else
			get = escapeId(key);

		return get;		
	}

	const 
		sql = this,
		takes = [],
		drops = [];

	for ( var lhs in query ) {
		const 
			rhs = query[lhs];
		var
			jsonIndex = 0;
		
		if ( lhs == dropCard ) 
			drops.push( "Field NOT LIKE " + escape(rhs.replace(wildMatch,"%")) );
		
		else
		if ( rhs ) 
			takes.push( getJSON(rhs, get => jsons.push(lhs) ) + " AS " + lhs );
		
		else {
			const get = getJSON(lhs, get => jsons.push( `json${jsonIndex}` ));
			takes.push( get.startsWith("json_extract") ? `${get} AS json${jsonIndex++}` : get );
		}
	}

	if ( drops.length )
		sql.query( `SHOW COLUMNS FROM ?? WHERE greatest(${drops},0)`, [ds], (err,recs) => {

			//Log(">>>fields", err);
			if (err)
				cb( "", [] );

			else {
				recs.forEach( rec => takes.push( escapeId(rec.Field) ) );
				cb( takes.join(","), jsons );	// empty takes?
			}
		});
	
	else
		cb( takes.join(",") || "*", jsons );
}

function Hawk(log) {  // journal changes 
	var sql = this;
	
	sql.query("SELECT * FROM openv.hawks WHERE least(?,Power)", log)
	.on("result", function (hawk) {
		sql.query(
			"INSERT INTO openv.journal SET ? ON DUPLICATE KEY UPDATE Updates=Updates+1",
			Copy({
				Hawk: hawk.Hawk,  	// moderator
				Power: hawk.Power, 	// moderator's level
				Updates: 1 			// init number of updates made
			}, log), err => {
				Log("journal", err);
		});
	});
}

/**
	ingest a comma-delimited, column-headered stream at path using the supplied
	streaming options.  Records are inserted into the sql target table defined 
	by the path = /.../target.type.  The keys="recKey:asKey sqlType,..." defines
	how record values are stored.
	
	@param {String} path source file
	@param {Object} opts {keys,comma,newline,limit,as,batch} streaming options
	@param {Function} cb Callback([record,...])
*/
function ingestFile(path, opts, filter) {

	const 
	 	sql = this,
		[ target ] = path.split("/").pop().split("."),
		{ batch,comma,newline,limit } = opts,
		makes = [],
		{ rekey,keys } = streamOpts = {
			batch: batch, 
			comma: comma || ",",
			newline: newline,
			rekey: filter ? [ `(${filter})=>!test` ] : [],
			limit: limit,
			keys: [] // csv file with unkown header
		};

	Trace( `INGESTING ${path} => ${target}` );
	opts.keys.forEach( (key,i) => {
		var 
			[ id,type ] = key.split(" ");

		rekey.push( id );
		makes.push( id + " " + type );
	});

	Log("INGEST", path, "=>", target);

	sql.query( `CREATE TABLE IF NOT EXISTS app.?? (ID float unique auto_increment,${makes.join(", ")})`, 
		[target], err => {
		
		Log( "INGESTING", target, err || "ok" );

		streamOpts.rekey = streamOpts.rekey.join(",");
		path.streamFile(streamOpts, recs => {
			if ( recs )
				recs.forEach( (rec,idx) => {
					sql.query("INSERT INTO app.?? SET ?", [target,rec] );
				});

			else 
				Trace( "INGESTED", path );
		});
	});
}

function Batch(table, {batch, limit, filter}, cb) {

	var sql = this;
	
	if ( batch ) 
		sql.query( "SELECT count(id) AS recs FROM app.??", table, (err,info) => {
			var recs = recs = limit || info[0].recs;
			//Log("=============== batching", recs );
			if ( recs ) 
				for (	var offset=0,reads=0; offset<recs; offset+=batch ) {
					//Log("=====================", offset,recs);
					sql.query( filter
						? "SELECT * FROM app.?? WHERE least(?,1) LIMIT ? OFFSET ?"
						: "SELECT * FROM app.?? LIMIT ? OFFSET ?", 

						filter
						? [table,filter,batch,offset]
						: [table,batch,offset], (err,data) => {

							cb(data, reads += batch);

							if ( reads >= recs ) {  // signal end
								//Log("================= ending", reads,recs);
								cb(null);
							}
						});
				}

			else	// signal end
				cb(null);
		});

	else
		sql.query( filter
			? "SELECT * FROM app.?? WHERE least(?,1)"
			: "SELECT * FROM app.??", [table,filter], (err,recs) => {

			cb( recs );
			cb( null ); // signal end
		});			
}
	
//================ form entry 

function relock(unlockcb, lockcb) {  //< lock-unlock record during form entry
	var 
		sql = this,
		ctx = this.ctx,
		ID = ctx.query.ID,
		lockID = {Lock:`${ctx.from}.${ID}`, Client:ctx.client};

	if (ID)
		sql.query(  // attempt to unlock a locked record
			"DELETE FROM openv.locks WHERE least(?)", 
			lockID, (err,info) => {

			if (err)
				ctx.err = JSDB.errors.failLock;

			else
			if (info.affectedRows) {  // unlocked so commit queued queries
				unlockcb();
				sql.query("COMMIT");  
			}

			else 
			if (lockcb)  // attempt to lock this record
				sql.query(
					"INSERT INTO openv.locks SET ?",
					lockID, (err,info) => {

					if (err)
						ctx.err = JSDB.errors.isLocked;

					else
						sql.query( "START TRANSACTION", err => {  // queue this transaction
							lockcb();
						});
				});	

			else  // record was never locked
				ctx.err = JSDB.errors.isUnlocked;

		});

	else
		ctx.err = JSDB.errors.noLock;
}

//================ url query expressions 

function whereify(query) {
	function proc( parms, op ) {
		for ( var key in parms ) {
			const 
				lhs = key.parseKEY( escapeId ),
				rhs = parms[key].parseKEY( escape ).replace(wildMatch,"%");

			if ( rhs.indexOf("%") >= 0 ) 
				switch (op) {
					case "=":
						ex.push( `${lhs} LIKE ${rhs}` );
						break;
					
					case "!=":
						ex.push( `${lhs} NOT LIKE ${rhs}` );
						break;
				}
			
			else
				switch ( op ) {
					case "!bin=":
						ex.push( `MATCH(${lhs}) AGAINST( '${rhs}' IN BOOLEAN MODE)` );
						break;
					case "!exp=":
						ex.push( `MATCH(${lhs}) AGAINST( '${rhs}' IN QUERY EXPANSION)` );
						break;
					case "!nlp=":
						ex.push( `MATCH(${lhs}) AGAINST( '${rhs}' IN NATURAL LANGUAGE MODE)` );
						break;
					default:
						ex.push( `${lhs} ${op} ${rhs}` );
				}
		}
	}
	
	var ex = [];
	for ( var op in query ) 
		proc( query[op], op );

	return ex.join(",");
}

//=============== query/fetch serialization

/**
	Serialize a select query.

	sql.serialize({
		ds1: "SELECT ... ",
		ds2: "SELECT ... ", ...
		ds3: "/dataset?...", 
		ds4: "/dataset?...", ...
	}, ctx, ctx => {
		// ctx[ ds1 || ds2 || ... ] records
	});
*/
function serialize( qs, opts, ctx, cb ) {	// legacy
	var 
		sql = this,
		qlist = [],
		fetchRecs = function (rec, cb) {
			var
				ds = rec.ds,
				query = rec.query;
			
			sql.query( 
				query, 
				ds.concat( rec.options || [] ), 
				(err, recs) => cb( err ? null : recs ) );
		};
	
	Each( qs, (ds,q)  => {
		qlist.push({
			query: q,
			ds: ds,
			options: opts
		});
	});
	
	qlist.serialize( fetchRecs, (q, recs) => {
		
		if (q) // have recs
			if (recs) 	// query ok
				if ( recs.forEach ) {  // clone returned records 
					var save = ctx[q.ds] = [];
					recs.forEach( rec => save.push( new Object(rec) ) );
				} 
		
				else  // clone returned info
					ctx[q.ds] = [ new Object(recs) ];
	
			else	// query error
				ctx[q.ds] = null;
	
		else  // at end
			cb( ctx );
	});
}

/*
function reroute( ds , ctx ) {  //< route ds=table||db_table to a protector 
	//var 
		//routes = JSDB.reroute,
		//[x,db,table] = ds.match(/(.*)_(.*)/) || [ "", "app", ds ],
		//ds = db + "." + table;
	
	if ( route = JSDB.reroute[ds] )
		return route(ctx || {} );
	
	else
		return "app."+ds;
}
*/

/*
function serialize( msg, query, args, cb ) {
	this.forAll( msg, query, args, (recs) => {
		recs.forEach( rec => cb(rec) );		// feed each record to callback
		cb(null);	// signal end
	});
}  */

/**
	Aggregate and save events evs = [ev, ...] || { } under direction of the 
	supplied context ctx = { Save: { ... }, Ingest: true||false, Export: true||false,
	... }.  Stashify is used to 
	aggreagate data using [ev, ...].stashify( "at", "Save_", ctx ) where events ev = 
	{ at: KEY, A: a1, B: b1, ... } || { x: x1, y: y1 } are saved in Save_KEY = 
	{A: [a1, a2,  ...], B: [b1, b2, ...], ...} iff Save_KEY is in the supplied ctx.  
*/
function saveContext(ctx) {	// save event context to plugin usecase

/**
Stash aggregated events evs = { at: "AT", ... } into context Save_AT keys then callback cb
with remaining events.

@param {object} sql sql connection
@param {object} evs events to be saved
@param {object} ctx notebook context
@param {function} cb callback(ev,stat) 
*/
	function saveEvents( sql, evs, ctx, cb ) {

/**
Aggregate ctx keys into optional Save_KEY stashes such that:

	[	
		{ at: "KEY", A: a1, B: b1, ... }, 
		{ at: "KEY", A: a2, B: b2, ... }, ... 
		{ x: x1, y: y1 },
		{ x: x2, y: y2 },	...
	].stashify( "at", "Save_", {Save_KEY: {}, ...} , stash, cb )

creates stash.Save_KEY = {A: [a1, a2,  ...], B: [b1, b2, ...], ...} iff Save_KEY is in the
supplied context ctx.   If no stash.rem is provided by the ctx, the {x, y, ...} are 
appended (w/o aggregation) to stash.remainder. Conversely, if ctx contains a stash.rem, 
the {x, y, ...} are aggregated to stash.rem.

@param {object} evs events to be saved
@param {String} watchKey  this = [ { watchKey:"KEY", x:X, y: Y, ...}, ... }
@param {String} targetPrefix  stash = { (targetPrefix + watchKey): { x: [X,...], y: [Y,...], ... }, ... } 
@param {Object} ctx plugin context keys
@param {Object} stash refactored output suitable for a Save_KEY
@param {Function} cb callback(ev,stat) returns refactored result to put into stash
*/
		function stashify(evs, watchKey, targetPrefix, ctx, stash, cb) {
			var rem = stash.remainder;

			evs.forEach( (stat,n) => {  // split-save all stashable keys
				var 
					key = targetPrefix + (stat[watchKey] || "rem"),  // target ctx key 
					ev = ( key in stash )
						? stash[key]  // stash was already primed
						: (key in ctx)  // see if its in the ctx
								? stash[key] = cb(null,stat, ctx[key]) // prime stash
								: null;  // not in ctx so stash in remainder

				if ( ev )  { // found a ctx target key to save results
					delete stat[watchKey];
					cb(ev, stat);
				}

				else  
				if (rem)  // stash remainder 
					rem.push( stat );
			});
		}

		function saveStash( sql, stash, ID, host ) {
			function saveKey( sql, key, save ) {
				try {
					sql.query(
						`UPDATE app.?? SET ${key}=? WHERE ID=?`, 
						[host, JSON.stringify(save) || "null", ID], 
						err => // will fail if key does not exist or mysql server buffer too small (see my.cnf)
							Trace(err ? `DROP ${host}.${key}` : `SAVE ${host}.${key}` )
					);
				}

				catch (err) {
					Trace( `DROP ${host}.${key}` )
					Log(err,save);
				}
			}

			for (var key in stash) 
				saveKey( sql, key, stash[key] );
		}

		function updateFile( sql, file, stats ) {
			stats.forEach( function (stat) {
				var save = {}, set=false;
				Each( stat, function (idx, val) {
					if ( idx in file) {
						save[ set = idx] = (typeof val == "object") 
							? JSON.stringify( val )
							: val;
					}
				});

				if (set)
					sql.query(
						"UPDATE openv.files SET ? WHERE ?",
						  [save, {ID: file.ID}],
						(err) => Log( err || "UPDATE "+file.Name)
					);
			});
		}

		function updateStats( sql, fileID, voxelID, stats ) {  // save relevant stats 
			var saveKeys = $.saveKeys;

			stats.forEach( function (stat) {
				var save = {}, set=false;
				Each( stat, function (key, val) {
					if ( key in saveKeys) 
						save[ set = key] = (typeof val == "object") 
							? JSON.stringify( val )
							: val;
				});

				if (set) 
					if (true) {
						save.fileID = fileID;
						save.voxelID = voxelID;					
						sql.query(
							"INSERT INTO openv._stats SET ? ON DUPLICATE KEY UPDATE ?",
							  [save, save] 
							// , (err) => Log( "STATS " + (err ? "FAILED" : "UPDATED") )
						);
					}

					else
						sql.query( "UPDATE openv.files SET ? WHERE ?", [save, {ID: fileID}] );

			});
		}

		//Log("save host", ctx.Host, ctx);

		//var evs = this;

		var 
			stash = { remainder: [] },  // stash for aggregated keys 
			rem = stash.remainder;

		stashify(evs, "at", "Save_", ctx, stash, (ev, stat) => {  // add {at:"KEY",...} evs to the Save_KEY stash

			if (ev)
				try {
					for (var key in stat) ev[key].push( stat[key] );
				}
				catch (err) {
					ev[key] = [ stat[key] ];
				}

			else {
				var ev = new Object();
				for (var key in stat) ev[key] = [ ];
				return ev;
			}

		});

		if (rem.length) {  // there is a remainder to save
			if (cb) cb(rem);

			saveStash(sql, {Save: rem}, ctx.ID, ctx.Host);	
		}

		delete stash.remainder;	

		if ( stash.Save_end ) 
			if ( stats = stash.Save_end.stats ) {   // there are stats that may need to be updated
				var
					file = ctx.File || {ID: 0},
					voxel = ctx.Voxel || {ID: 0};

				updateStats(sql, file.ID, voxel.ID, stats);
			}

			/*
			if ( File = ctx.File )
				updateFile( sql, File, stats);

			else
				sql.forFirst( "", "SELECT * FROM openv.files WHERE ? LIMIT 1", {Name: ctx.Host+"."+ctx.Name}, function (File) {
					if (File) 
						updateFile(sql, File, stats);
				});
			*/

		saveStash(sql, stash, ctx.ID, ctx.Host);

		return ctx.Share ? evs : "updated".link("/files.view");
	}

	const 
		sql = this,
		{ Host,Name,Export,Ingest } = ctx,
		{ savers } = JSDB,
		now = new Date(),
		client = "guest",
		savepath = `${Host}_${Name}`;

	sqlThread( sql => {
		Each(ctx, (key,store) => {
			//Log("save", key, "dump?", savers[key], Host, Name, savepath);
			
			if ( dump = savers[key] ) {
				Log("dumping", key);
				dump( savepath, store );
			}

			else
			if ( key == "Save" ) 
				saveEvents( sql, store, ctx, evs => {  // save events and callback with remaining unsaved evs

					if ( Export ) {   // export remaining events to filename
						var
							evidx = 0,
							srcStream = new STREAM.Readable({    // establish source stream for export pipe
								objectMode: false,
								read: function () {  // read event source
									if ( ev = evs[evidx++] )  // still have an event
										this.push( JSON.stringify(ev)+"\n" );
									else 		// signal events exhausted
										this.push( null );
								}
							});

						Trace("EXPORT "+savename);
						uploadFile( "", srcStream, savepath+".stream" );
					}

					if ( Ingest )  // ingest remaining events
						getBrick( client, savename+".stream", file => {
							sql.query("DELETE FROM openv.events WHERE ?", {fileID: file.ID});

							ingestList( sql, evs, file.ID, file.Class, aoi => {
								Log("INGESTED",aoi);

								if ( false )
									sqlThread( sql => {	// run plugins that were linked to this ingest
										exeAutorun(sql,"", `.${ctx.Host}.${ctx.Name}` );
									});
							});
						});

					sql.query(
						"UPDATE app.?? SET Save=? WHERE Name=?",
						[Host, JSON.stringify(evs), Name] );
				}); 

			else			
			if ( key.startsWith("Save") ) 
				sql.query(
					`UPDATE app.?? SET ${key}=? WHERE Name=?`,
					[Host, JSON.stringify( store ), Name], err => Log(err||`saved ${key}`) );
		});
	});

	/*if ( !isEmpty(ctx) )
		sql.query( 
			"UPDATE app.?? SET ? WHERE Name=?", 
			[Host, ctx, Name], 
			err => Log(">>>save", err?err:"ok", ctx) ); */

	return "Saved";
}
	
/**
Regulate task = {Client,Class,Task,Name,QoS,Sign0, ...} into jobs given a supplied clock and callbacks:

	feedcb( err, step ) to feed a record batch into the queue via the stepper step callback 
	taskcb( recs, ctx, res ) to process each record batch in the provided context ctx with responder res callback

When a feedcb is provided, the taskcb is placed into a stream workflow which terminates when the
records batch recs becomes null.  The res(save) callback must be called by the taskcb to advance the task: its 
save context ctx is optional.  The task context ctx is loaded from and saved into its Save_<Class> json store
at each step.

If no feedcb is provided, the taskcb is periodically executed with a null records batch; in this use-case,
the callback to res(save) is optional.

Tasks are identified by Class-Task-Name and increase their Run counter as they are reused.  

A nonzero QoS sets a tasking watchdog timer to manage the task.  

A creditless Client signals a non-null error to the feedcb.

To establish the task as a proposal, set Sign0 = 1.  In so doing, if (Sign1,Sign2,Sign3) are never signed-off 
before the proposal's start time, the task will be killed.
*/
function queueTask(clock,task,taskcb,feedcb) {
	// ==================== configuration options
	
	const
		snapshots = true,		// enable job snapshots
		profiles = "openv.profiles",
		queues = "openv.queues";
	
	// ===================== end configuration options
	
	function cpuUtil() {				// return average cpu utilization
		var avgUtil = 0;
		var cpus = OS.cpus();

		cpus.forEach( cpu => {
			idle = cpu.times.idle;
			busy = cpu.times.nice + cpu.times.sys + cpu.times.irq + cpu.times.user;
			avgUtil += busy / (busy + idle);
		});
		return avgUtil / cpus.length;
	}
	
	function memUtil() {
		return OS.freemem() / OS.totalmem();
	}
	
	function manageTask( sql, task ) {
		if ( task )
			if ( task.Kill ) 
				stopTask(sql, task.ID, true);
			else {
			}
		
		else // someone deleted the task
			stopTask( sql, 0, true );
	}
		
	function startTask( sql, cb ) {
		sql.query(
			"SELECT Credit FROM ?? WHERE Client=?", [profiles, Client], (err,profs) => {
				
			if ( prof = profs[0] )
				if ( prof.Credit )
					sql.query(  			// add job to the job queue
						"INSERT INTO ?? SET ? ON DUPLICATE KEY UPDATE Run=Run+1,?",
						[queues,entry,entry],
						(err,info) => {  	// increment work backlog for this job

						const
							taskID = err ? 0 : info.insertId;

						if ( taskID && watchdog ) 	// setup task watchdog
							watchTask = setInterval( jobs => {
								sqlThread( sql => {
									sql.query("SELECT * FROM ?? WHERE ID=?", [queues,taskID], (err,recs) => manageTask( sql, recs[0] ) );
								});
							}, watchdog, jobs );

						cb( taskID );
					});
				
				else	// client has no credit 
					cb( 0 );
				
			else	// no such client
				cb( 0 );
		});
	}
	
	function stopTask( sql, taskID, kill ) {
		if ( kill ) {								// kill all pending jobs and flush queue
			jobs.forEach( (job,idx) => {			// clear jobs
				clearTimeout( job.timer );
				delete jobs[ idx ];
			});
			jobs.length = 0;						// flush queue
		}
							
		if (watchTask) clearTimeout(watchTask);		// stop watching the task
		
		sql.query( 
			"UPDATE ?? SET Departed=now(),Finished=1 WHERE ID=?", 
			[queues,taskID] );
		
		sql.query(  			// update clients credit
			"UPDATE ?? SET Credit=Credit-? WHERE Client=?",
			[ profiles, batchesFed, Client ] );
	}

	function updateTask( sql, taskID ) {
		sql.query(  			// update job queue
			"UPDATE ?? SET " +
			"Done=Done+1, " +
			"Age=datediff(now(),Arrived)*24, " + 
			"State=Age/Done, " +
			"ECD=?,Snaps=Snaps+1,Events=?,Batches=? WHERE ID=?", 
			[ queues, clock.next, eventsFed, batchesFed, taskID ] );		// "ECD=date_add(Arrived, interval State*Work hour), "
	}
	
	const 
		sql = this,
		traceQueue = ( msg, args ) => `task-${Class}`.trace( msg, null, msg => console.log( msg, args )),
		jobs = [],
		{ end } = clock,
		{ Client, Name, Task, QoS, Class } = entry = Copy(task, {
			// mysql unique keys should not be null
			Client: "tbd",
			Class: "tbd",
			Task: "tbd",
			Name: "tbd",

			// job qos
			QoS: 0,
			Priority: 0,

			// others 
			Notes: "",
			Arrived	: new Date(),
			Departed: null,
			Classif : "(U)",

			// flags
			Kill: 0,
			Billed: 0,
			Flagged: 0,
			Finished: 0,
			Funded: 1,

			// signoffs
			Sign0: 0,
			Sign1: 0,
			Sign2: 0,
			Sign3: 0,

			// metrics
			Events: 0,
			Batches: 0,
			Snaps: 0,

			// Completion estimates
			Age: 0,
			State: 0,
			ECD: null,
			cpuUtil: cpuUtil(),
			memUtil: memUtil(),
			Work: 0,
			Done: 1
		}),
		book = Task,
		usecase = Name,
  		watchdog = QoS*1e3 || 0;	// task watchdog interval (0 disables)  in msecs

	var							// task metrics
		snapStore = "Save_snap", // "Save_"+Class,	// snapshot json store
		watchTask = 0,			// watchdog timer
		eventsFed = 0,			// # of records routed to taskcb
		batchesFed = 0,			// # of bacthes routed to taskcb
		eventsQueued = 0,		// # of records queued by feedcb
		batchesQueued = 0;		// # of batches queued by feedcb
	
	startTask( sql, taskID => {
		function stepTask( idx ) {

			function startSnap( cb ) {
				sqlThread( sql => {
					sql.query(		// check approval status
						"SELECT Sign0,Sign1,Sign2,Sign3 FROM ?? WHERE ID=?",
						[ queues, taskID ],
						(err,tasks) => {
							
						//Log("start", err, tasks);
						if ( task = tasks[0] ) {
							const {Sign0,Sign1,Sign2,Sign3} = task;

							if ( Sign0 && (!Sign1 || !Sign2 || !Sign3) ) {	// signers have rejected task proposal so ...
								stopTask( sql, taskID, true );		// kill task
								cb( sql, null );					// signal task dead
							}

							else {
								//sql.query("START TRANSACTION");
								sql.query(
									`SELECT ${snapStore} AS ctx FROM app.?? WHERE Name=? `, // + "FOR UPDATE", 
									[book,usecase], 
									(err,snaps) => {
										
										//Log("start2", err, snaps);
										//Log("...............load snap", snapStore, snaps);
										
										if ( err ) 	// notebook did not define a snapshot store
											cb( sql, null );
										
										else
										if ( snap = snaps[0] )
											cb( sql, JSON.parse( snap.ctx ) || {} );
										
										else	// someone deleted usecase
											cb( sql, null );
									});
							}
						}
							
						else  	// someone deleted the task
							cb( sql, null );
					});
				});
			}

			function saveSnap( sql, save ) {
				const 
					savectx = {
						Host: book,
						Name: usecase
					};

				//Log(".............save snap", savectx);
				savectx[snapStore] = save;
				Each( save, (key,val) => {
					//Log(".........dump?", key);
					if ( key.startsWith("_") ) {
						//Log(".............special dump key", key);
						savectx[key] = val;
						delete save[key];
					}
				});
				sql.saveContext(savectx);
			}

			function stopSnap( sql, ctx, kill ) {
				stopTask( sql, taskID, kill );
				/*taskcb( null, ctx, save => {
					if ( save ) 
						saveSnap(sql,save);
				});*/
			}
			
			//Log("====step", idx);
			
			startSnap( (sql,ctx) => {	// load previous snapshot context
				
				//Log("====step ctx", idx, ctx?true:false);
				
				if ( ctx ) {			// valid context so step the task
					const
						recs = jobs[idx].recs;

					if ( recs ) {	// advance metrics if this is a batch
						//Log("====step start", idx, "recs", recs.length);
				
						eventsFed += recs.length;
						batchesFed ++;
						traceQueue( "drain", {batches: batchesFed, events: eventsFed});
					}
					
					taskcb( recs, ctx, save => {	// call task then save its snapshot

						//Log("====step start snap",idx);
						traceQueue( "percent complete", doneJobs/jobs.length );

						//Log("task save?", save?true:false);
						if ( save ) 	// update and reset snapshot metrics
							saveSnap(sql,save);

						delete jobs[idx];		// free job records

						if ( end && (new Date() >= end) )  		// job past its PoP so issue hard halt
							stopSnap( sql, ctx, true );

						if ( ++doneJobs >= jobs.length ) 		// no more jobs so issue soft halt
							stopSnap( sql, ctx, false);

						else
							updateTask( sql, taskID );

						//sql.query("COMMIT");
					});	
				}
				
				else 					// no context (task rejected) so kill the task
					stopTask( sql, taskID, true );
					
			});
		}

		var		// track # jobs completed
			doneJobs = 0;

		if ( taskID )			// task succesfully started
			if ( feedcb ) 		// requesting regulated task
				feedcb( null, recs => {	// get a batch of records
					if ( recs ) {		// queue this record batch
						const
							Recs = recs.forEach ? [] : recs;	// retain either the recs batch or the recs file path
							
						if ( recs.forEach ) 		// clone record batch
							recs.forEach( rec => Recs.push( new Object(rec) ));

						//Log("PUSH BATCH", jobs.length);
						jobs.push({				// add a job to the task queue
							recs: Recs,
							idx: jobs.length,
							timer: 	setTimeout( 
								idx => {
									//Log("DO BATCH", idx); 
									stepTask( idx );
								}, 
								
								clock.tick( wait => {	// add a snapshot signal 

									//Log("PUSH SNAP", jobs.length, wait);
									jobs.push({	// callback notebook with null batch to signal snapshot
										recs: null,
										idx: jobs.length,
										timer: setTimeout( idx => {
												//Log("DO SNAP", idx);
												stepTask( idx );
											}, 

											wait, 

											jobs.length )
									});	
									
								}), 
								
								jobs.length )
						});
						
						batchesQueued++;
						eventsQueued += recs.forEach ? recs.length : 1;
						traceQueue("queue", {batches: batchesQueued, events: eventsQueued});
					}

					else {	// prime task and schedule EOS job
						jobs.push({				// add a job to the task queue
							recs: null,
							idx: jobs.length,
							timer: setTimeout( 
								idx => {
									//Log("BATCH", idx); 
									stepTask( idx );
								}, 
								
								clock.tick( ), 
								
								jobs.length )
						});
						sql.query(
							"UPDATE ?? SET Work=?,ECD=date_add(Arrived, interval State*Work hour) WHERE ID=?", 
							[queues, jobs.length,taskID] );
					}
				});

			else {				// requesting unregulated task
				setTimeout( 
					idx => {		// initial delay to syncup on requested interval
						jobs.push({
							recs: null,
							idx: jobs.length,
							timer: setInterval( 
									idx => taskcb( null, jobs[idx], save => {
										//Log(">>>job nofeed", clock.tick(), clock.every);
										updateTask(sql,taskID);
									}),  

									clock.tick(), 

									jobs.length )
						});
					}, 
					
					clock.tick( ), 
					
					0);
			}
		
		else				// task could not be started
			if ( feedcb ) 
				feedcb( new Error(`client ${Client} is broke`) );

			else
				Log( new Error(`client ${Client} is broke`) );
	});
}

[
	function parseKEY( escape ) {
		const 
			[x,lhs,op,rhs] = this.match( /(.*?)(\$)(.*)/ ) || [];
		
		//Log(this,x,[lhs,op,rhs]);
		
		if ( x ) 
			if (lhs) {
				var idx = rhs.split(",");
				idx.forEach( (key,n) => idx[n] = escape( n ? key : op+key) );
				return `json_extract(${escapeId(lhs)}, ${idx.join(",")} )`;
			}

			else
				return escapeId(rhs);
		
		else
			return escape(this+"");
		/*
		return this.parseOP( /(.*?)(\$)(.*)/, key => escape(key) , (lhs,op,rhs) => {
			
			if (lhs) {
				var idx = rhs.split(",");
				idx.forEach( (key,n) => idx[n] = escape( n ? key : op+key) );
				return `json_extract(${escapeId(lhs)}, ${idx.join(",")} )`;
			}

			else
				return escapeId(rhs);
		}); */
		
	}
].Extend(String);

/**
@class JSDB.Unit_Tests_Use_Cases
*/

switch ( process.argv[2] ) { //< unit tests
	case "?":
		Log("unit test with 'node jsdb.js [B1 || B2 || ...]'");
		break;
		
	case "N1":
		JSDB.config( null, sql => {
			neoThread( neo => {	// add prototypes and test neo4j connection
				if (false) // test connection
					neo.cypher(
						// 'MATCH (u:User {email: {email}}) RETURN u',
						'MERGE (alice:Person {name : $nameParam}) RETURN alice.name AS name', {
							// email: 'alice@example.com',
							nameParam: 'Alice'
					}, (err, recs) => {
						if (err) 
							Log( err );

						else 
							if ( rec = recs[0] ) 
								Log("neodb test alice user", rec );
										// JSON.stringify(rec['u'], null, 4));

							else
								Log('neodb test - alice has no records.');
					});

				if (false) // clear db on startup
					neo.cypher(
						"MATCH (n) DETACH DELETE n", {}, err => {
						Log( err || "CLEAR GRAPH DB" );
					});  
			});
		});

		break;
		
	case "S1":
		/*
		opts = {
			// login credentials
			host: "mysqlhost",
			user: "root",
			password: "root",
			port: 3306,

			// connection options
			connectionLimit: 50,	// max number to create "at once" - whatever that means
			acquireTimeout: 600e3,	// ms timeout during connection acquisition - whatever that means
			connectTimeout: 600e3,	// ms timeout during initial connect to mysql server
			queueLimit: 0,  						// max concections to queue (0=unlimited)
			waitForConnections: true,		// queue when no connections are available and limit reached


			// reserved for ...
			threads: 0, 	// connection threads
			pool: null		// connector
		};

		pool=MYSQL.createPool(opts);

		pool.query("SELECT 123", (err,recs) => console.log(recs) );
		pool.query("SELECT * from openv.apps", (err,recs) => console.log(recs) );
		*/
		JSDB.config( null, sql => {
			Trace("SQLDB "+(sql?"connected":"disconnected"));
		});
		
		break;
}
